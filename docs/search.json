[
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Viewed from Above: An Learning Diary on Remote Sensing & Earth Observation",
    "section": "About me",
    "text": "About me\n\n\n\n\n\n\nXin chào!\nMy name is The-Huan, although I also go by Huan or Shaun, depending on where in the world you see me. Hailing from Vietnam, I have lived, studied, and worked in many countries like the United States, Argentina, Denmark, Italy, and Singapore. Most recently though, I can be found thrift-shopping in London.\nI am an economist by training with a long stint in the Marketing Tech industry. While on the job, I was fascinated by the power of big data and the cloud. Marrying that with my love for maps and the desire to see more of the world in ways I have not been to previously, I decided to embark on a journey into Remote Sensing and Earth Observation.\nI also love talking about (and in) foreign languages that I know and am learning. Currently working on German and Mandarin.\n\n\nGet in touch\nEmail, LinkedIn"
  },
  {
    "objectID": "week1.html#summary",
    "href": "week1.html#summary",
    "title": "1  Intro to Remote Sensing",
    "section": "1.1 Summary",
    "text": "1.1 Summary\nRemote Sensing uses satellites, planes, drones, etc., as our aerial eyes, piecing together a portrait of our planet through light and data and revolutionising how we understand and interact with it.\nSensors, well, ‘sense’ Earth in two ways: Some passively listen to sunlight reflected off our planet’s surface (i.e., passive sensors), while others actively send their own signals and capture the echoes (i.e., active sensors). By that definition, the human eye is a type of passive sensors!\n\n\n\nActive vs Passive Remote Sensing (Earth Science Data Systems 2019).\n\n\nThese sensors interpret a fascinating language of light, both invisible and visible, known as the electromagnetic spectrum.\n\n\n\nThe Electromagnetic Spectrum (EMS). Credit: NASA Science\n\n\nElectromagnetic radiation moves as waves as perpendicular electric and magnetic field with a wavelength: λ = c/v where:\n\nλ = wavelength, the distance between two crests\nc = velocity of light 3 x 108 m/sec\nv = frequency, rate of oscillation (full oscillations in a time unit)\n\nDifferent materials reflect unique wavelengths in this spectrum, allowing us to identify them, like decoding DNA!\n\n\n\nWavelength vs. Oscillation\n\n\nBut the information that sensors receive isn’t just about colour. Remote sensing data has its own “resolution” recipe, encompassing:\n\nSpectral: How EMS bands (i.e., the range within the EMS) the sensor can hear, revealing more detail with each additional band.\nSpatial: The size of each pixel in the image, ranging from centimetres to kilometres, offers varying detail levels.\nTemporal: How often the sensor revisits the same area, providing a dynamic view of changes over time.\nRadiometric: The range of brightness levels captured, painting a vibrant and accurate picture.\n\nDepending on the purpose, each sensor is equipped to have better resolution of one type over the other. For example, sensors with a high spatial resolution of 5m (i.e., each pixel is 10x10 on the ground) will have a lower spectral resolution (i.e., capturing only a narrow range of the EMS) (“Remote Sensing, Satellite Imaging Technology | Satellite Imaging Corp” n.d.)"
  },
  {
    "objectID": "week1.html#application",
    "href": "week1.html#application",
    "title": "1  Intro to Remote Sensing",
    "section": "1.2 Application",
    "text": "1.2 Application\nRemote Sensing has many transformative applications in the realm of Urban Analytics. Think of it as an X-ray for cities, revealing hidden patterns and empowering informed decision-making by city planners, urban designers, and public officials to make swift and informed decisions to improve the lives of millions of urbanites.\nHere are some examples of the use of Remote Sensing in urban analytics research\n\nMapping Urban Growth: By tracking changes in the built environment over time, we can identify sprawling suburbs, monitor urban expansion, and plan for infrastructure needs. Recent works have used a diverse source of high-resolution Remote Sensing data (2m) to train a machine-learning model to extract ‘urbanised’ areas more robustly compared to a previous methodology using only medium-resolution images. (Wang et al. 2021)\n\n\n\n\nTransferable built-up area extraction (TBUAE) framework to map urbanised areas\n\n\n\nPredicting Floods with Foresight: Analysing land cover and terrain, it anticipates where water will flow, safeguarding communities from harm while also estimating potential damage. This type of risk assessment is highly relevant not only in climate science and the public sector but also among insurance companies who make use of SAR satellite data (for through-cloud vision) combined with other spatial data sets to assess risk and process claims (Schumann et al. 2023)\n\n\n\n\nRemote sensing data can be used to estimate flood extent and to derive individual risk level damage estimates.\n\n\n\nEnergy Efficiency: Identifying heat island effects and understanding building energy consumption through thermal imaging empowers planners to design sustainable cities and researchers. Remote Sensing data is a key component in Urban Climate Models such as UrbCLIM by VITO, a Belgian research organisation, which aims to create an interactive tool providing high-resolution urban heat maps and predict their evolution under future climate conditions. (“DestinE for Human Heat Stress: ECMWF Use Case to Tackle Urban Heat Islands” n.d.)\n\n\n\n\nLeft: Average 2m air temperature at 23h (moment of max Urban Heat Islands) during all summer months of 1987- 2016. Right: UrbCLIM climate model’s output field showing the average daily maximum urban heat island intensity for Amsterdam\n\n\nThis is just the beginning. Remote sensing transforms how we understand and manage our cities, paving the way for a healthier, smarter, and more sustainable urban future."
  },
  {
    "objectID": "week1.html#reflection",
    "href": "week1.html#reflection",
    "title": "1  Intro to Remote Sensing",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\nMy first foray into the world of Remote Sensing was eye-opening. For the uninitiated, it is easy to assume that remote sensing purely means orthophotography satellite images that one might see using platforms such as Google Maps, i.e. as if the only thing that sensors do were to snap a simple photo of the planet like a phone camera.\nIn reality, it unlocks unseen depths of data beyond mere human perception, not just high-resolution photographs and more spectral signatures, where each pixel holds a universe of information. Building a true-colour composite from all these layers (so that our limited human vision can perceive them) was like seeing the world through a brand-new lens.\nI am particularly excited to get started with Google Earth Engine later on as the primary gateway to access the wealth of remote sensing data and analytics with more ease to solve specific problems facing our world today.\n\n\n\n\n“DestinE for Human Heat Stress: ECMWF Use Case to Tackle Urban Heat Islands.” n.d. Accessed January 29, 2024. https://stories.ecmwf.int/destine-for-human-heat-stress-ecmwf-use-case-to-tackle-urban-heat-islands/.\n\n\nEarth Science Data Systems, NASA. 2019. “What Is Remote Sensing? | Earthdata.” Backgrounder. August 23, 2019. https://www.earthdata.nasa.gov/learn/backgrounders/remote-sensing.\n\n\n“Remote Sensing, Satellite Imaging Technology | Satellite Imaging Corp.” n.d. Accessed January 27, 2024. https://www.satimagingcorp.com/services/resources/characterization-of-satellite-remote-sensing-systems/.\n\n\nSchumann, Guy, Laura Giustarini, Angelica Tarpanelli, Ben Jarihani, and Sandro Martinis. 2023. “Flood Modeling and Prediction Using Earth Observation Data.” Surveys in Geophysics 44 (5): 1553–78. https://doi.org/10.1007/s10712-022-09751-y.\n\n\nWang, Haibo, Xueshuang Gong, Bingbing Wang, Chao Deng, and Qiong Cao. 2021. “Urban Development Analysis Using Built-up Area Maps Based on Multiple High-Resolution Satellite Data.” International Journal of Applied Earth Observation and Geoinformation 103 (December): 102500. https://doi.org/10.1016/j.jag.2021.102500."
  },
  {
    "objectID": "week2.html#or-how-to-3d-print-a-city",
    "href": "week2.html#or-how-to-3d-print-a-city",
    "title": "2  A peek into the LiDAR technology",
    "section": "Or, how to 3D print a city",
    "text": "Or, how to 3D print a city\nHaving understood how sensors work in theory, we can now shift our view to appreciate how one of them in particular, LiDAR (Light Detection and Ranging), is deployed in practice and can benefit us not only in an emerging sector like Autonomous Vehicles but also in Urban Planning"
  },
  {
    "objectID": "week3.html#summary",
    "href": "week3.html#summary",
    "title": "3  Corrections and Enhancements",
    "section": "3.1 Summary",
    "text": "3.1 Summary\nRaw data from sensors are rarely immediately usable without being corrected for various interferences and effects. Common correction methods are geometric and topographic corrections and radiometric and atmospheric corrections. Finally, enhancement methods help us highlight values of interest that pertain to our research scope.\n\n3.1.1 Radiometric and Atmospheric corrections\nThese processes describe translating raw light data from the sensor into ‘true’ information on the surface’s reflectance property without interference from the light source and the atmosphere.\n\nRadiometric calibration is the conversion from raw Digital Number (raw, no units) to Spectral Radiance via a linear transformation \\(L_λ=Bias+(Gain∗DN)\\). Radiance most often has units of watt/(steradian/square meter)\nAtmospheric correction is the next step:\n\nTOA Radiance-to-Reflectance correction removes the effects of the light source (e.g. the sun) by calibrating radiation going down (irradiance) and up (radiance). TOA Reflectance still affects the atmosphere and the surface material. If irradiance is equal to radiance, we call this hemispheric reflectance.\nTOA-to-BOA Reflectance correction removes the effects of the atmospheric conditions, leaving us with just data on the surface materials. If shadows and directional effects on reflectance have been dealt with, we get what is called true reflectance; if not, it is called apparent reflectance.\n\n\nAtmospheric correction deserves our attention, considering the effect of atmospheric scattering on the final results. Absorption and scattering create the haze, which reduces the contrast and can create the “adjacency effect”, whereby radiance from pixels nearby is mixed into pixels of interest. Atmospheric correction to obtain actual reflectance is not always necessary, for example, for classifying a single image, working on composite images, etc. There are two types of atmospheric correction: -\n\nRelative: normalise intensities of different bands within a single image or from many dates to one date. This can be done via Dark Object Subtraction (DOS) or Pseudo-invariant Features (PIFs).\nAbsolute: change digital brightness values into scaled surface reflectance. We can then compare these scaled surface reflectance values across the planet through atmospheric radiative transfer models (i.e. summer, tropical) or Empirical Line Correction.\n\n\n\n\nExample of LEDAPS atmospheric correction. (a) Top-of-atmosphere (TOA) reflectance composite (bands 3,2,1) for Landsat-7 ETM+ image of San Francisco Bay (July 7, 1999); (b) Surface reflectance composite.\n\n\n\n\n3.1.2 Geometric and topographic corrections\nThese are subsets of Georectification, which gives coordinates to an image and accounts for view angle, topography, wind disturbance, Earth rotation, etc., distorting the resultant image’s geometry.\nTopographic correction corrects the view angle of the image so that it is nadir (i.e., directly top-down). Important concepts to get familiar with for orthorectification are:\n\nSolar azimuth: compass angle of the sun (N =0°) 90° (E) at sunrise and 270° (W) at sunset.\nSolar zenith: angle of local zenith (above the point on the ground) and sun from vertical (90° - elevation)\n\n\n\n\n\n\nGeometric correction effectively ‘grounds’ the images into a georeferenced final product (i.e., with a coordinate). We identify Ground Control Points (GPS) to match known points in the image and a reference dataset. We then model the coordinates to give geometric transformation coefficients (linear regression). It effectively resembles fitting old maps into a digital version.\n\n\n\n\n\n\n\n3.1.3 Joining and Enhancements\nTo join (i.e., ‘mosaicking’), within the overlap area (20-30%), a representative sample is taken, a histogram is extracted from the base image, which is then applied to other images using a histogram matching algorithm to blend the brightness values of the two images (‘feathering’)\nFinally, after all the corrections, there are still many enhancements that can improve or accentuate the visual results depending on the purpose of the research:\n\nContrast enhancement to accentuate reflectance values that are close to each other\nRatio calculation calculates pixel value as a ratio of different bands (e.g. Normalised Burn Ratio)\nFiltering is the use of low-pass filters that average (i.e., smooth) the data or high-pass filters that enhance the variances between features. Filtering is used to perform texture and edge detection.\nPCA transforms multi-spectral data into uncorrelated datasets. Multi-date overlay PCA is a way to detect change efficiently.\nFusion entails fusing images/data from multiple sensors to improve details, enable better classification, or downsample."
  },
  {
    "objectID": "week3.html#application",
    "href": "week3.html#application",
    "title": "3  Corrections and Enhancements",
    "section": "3.2 Application",
    "text": "3.2 Application\nDespite having delved a lot into corrections, it is worth noting that standard remote sensing products now come corrected (Level-2, or “Analysis Ready Data”, ARD). In contrast, products derived from corrected ones are called Level-3. Research that works directly with Level-1 products often seeks to refine the correction methodology to transform them into ARDs (Coluzzi et al. 2018)\nEnhancements made to ARD (i.e. Level-3 products) represent a proliferating field to survey, thanks to its wealth of innovative applications of a single or combinations of techniques, depending on the task at hand. According to recent research that made use of remote sensing data, there seem to be two main umbrella objectives researchers have when considering which enhancement methods to employ:\n\nVisualisation: Visual enhancement essentially accentuates the desired subject vs. other details. Contrast stretching to make images appear brighter is often done without much fanfare but is an essential step in using remote sensing data as an artefact perceptible in print to the human eyes. However, image enhancement using band ratio is widely used to highlight certain objects, with indices including NDVI, SAVI, etc., for Vegetation or NDWI, SWI, etc., for Water and Snow. A combination of indices can also be used to produce a composite ratio that can best visualise the desired study area, which was how Macedo et al. (2018) was able to map the holm oak above-ground biomass over a large area with different atmospheric conditions.\nFeature extraction: Many researchers seek to extract novel datasets from remote sensing data for various purposes, including training machine learning models to do the same (and better) for a larger region or globally. This objective is related to but ultimately distinguished from the above because the output is not a cartographic product but a dataset. The main difficulty when tackling this is to classify accurately (avoid false negatives and positives) while retaining the depth of information in each pixel. Li et al. (2022) proposed a pyramid feature extraction (PFE) to construct multi-scale representations of buildings, in which convolutional neural networks were applied on satellite images already gone through a combination of edge and texture detection, which were then again applied to subsequent output in the workflow."
  },
  {
    "objectID": "week3.html#reflection",
    "href": "week3.html#reflection",
    "title": "3  Corrections and Enhancements",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection\nMy initial knowledge of remote sensing was limited, but this exploration proved insightful and rewarding. The vast amount of information obtainable from raw multi-spectral data is impressive and somewhat little-appreciated outside of the geospatial community.\nTherefore, democratising remote sensing data and technological advancements will empower research on Earth’s surface, surpassing local data collection, which is uneven and ununified by nature. Instead, remote sensing can be performed globally if the correct adjustments for the atmosphere and enhancements to fit the objectives are made.\nHowever, while this is an untapped data source, there are two hurdles to overcome before they may be fully utilised.\n\nTechnical competence: Remote sensing is jargon-filled, with each discipline adopting its best practice to harness the data, making the barrier to entry higher than that of other types of data analysis work. Standardisation of industry-agnostic workflow and training may be vital to upskilling geospatial analysts to work more natively with remote sensing data.\nHigh-resolution EO data are mostly not free and reserved for governmental/military use. High-quality EO data (multispectral, high spatial resolution, frequent, etc.) also depends on localities. Sophisticated enhancement techniques may help bridge the gap by fusing existing datasets.\n\n\n\n\n\nColuzzi, Rosa, Vito Imbrenda, Maria Lanfredi, and Tiziana Simoniello. 2018. “A First Assessment of the Sentinel-2 Level 1-C Cloud Mask Product to Support Informed Surface Analyses.” Remote Sensing of Environment 217 (November): 426–43. https://doi.org/10.1016/j.rse.2018.08.009.\n\n\nLi, Wangbin, Kaimin Sun, Hepeng Zhao, Wenzhuo Li, Jinjiang Wei, and Song Gao. 2022. “Extracting Buildings from High-Resolution Remote Sensing Images by Deep ConvNets Equipped with Structural-Cue-Guided Feature Alignment.” International Journal of Applied Earth Observation and Geoinformation 113 (September): 102970. https://doi.org/10.1016/j.jag.2022.102970.\n\n\nMacedo, Fabrício L., Adélia M. O. Sousa, Ana Cristina Gonçalves, José R. Marques da Silva, Paulo A. Mesquita, and Ricardo A. F. Rodrigues. 2018. “Above-Ground Biomass Estimation for Quercus Rotundifolia Using Vegetation Indices Derived from High Spatial Resolution Satellite Images.” European Journal of Remote Sensing 51 (1): 932–44. https://doi.org/10.1080/22797254.2018.1521250."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Coluzzi, Rosa, Vito Imbrenda, Maria Lanfredi, and Tiziana Simoniello.\n2018. “A First Assessment of the Sentinel-2 Level\n1-C Cloud Mask Product to Support Informed Surface\nAnalyses.” Remote Sensing of Environment 217 (November):\n426–43. https://doi.org/10.1016/j.rse.2018.08.009.\n\n\n“DestinE for Human Heat Stress: ECMWF\nUse Case to Tackle Urban Heat Islands.” n.d. Accessed January 29,\n2024. https://stories.ecmwf.int/destine-for-human-heat-stress-ecmwf-use-case-to-tackle-urban-heat-islands/.\n\n\nEarth Science Data Systems, NASA. 2019. “What Is Remote\nSensing? | Earthdata.” Backgrounder. August\n23, 2019. https://www.earthdata.nasa.gov/learn/backgrounders/remote-sensing.\n\n\nLi, Wangbin, Kaimin Sun, Hepeng Zhao, Wenzhuo Li, Jinjiang Wei, and Song\nGao. 2022. “Extracting Buildings from High-Resolution Remote\nSensing Images by Deep ConvNets Equipped with\nStructural-Cue-Guided Feature Alignment.” International\nJournal of Applied Earth Observation and Geoinformation 113\n(September): 102970. https://doi.org/10.1016/j.jag.2022.102970.\n\n\nMacedo, Fabrício L., Adélia M. O. Sousa, Ana Cristina Gonçalves, José R.\nMarques da Silva, Paulo A. Mesquita, and Ricardo A. F. Rodrigues. 2018.\n“Above-Ground Biomass Estimation for Quercus\nRotundifolia Using Vegetation Indices Derived from High Spatial\nResolution Satellite Images.” European Journal of Remote\nSensing 51 (1): 932–44. https://doi.org/10.1080/22797254.2018.1521250.\n\n\n“Remote Sensing, Satellite Imaging\nTechnology | Satellite Imaging Corp.” n.d.\nAccessed January 27, 2024. https://www.satimagingcorp.com/services/resources/characterization-of-satellite-remote-sensing-systems/.\n\n\nSchumann, Guy, Laura Giustarini, Angelica Tarpanelli, Ben Jarihani, and\nSandro Martinis. 2023. “Flood Modeling and\nPrediction Using Earth Observation Data.”\nSurveys in Geophysics 44 (5): 1553–78. https://doi.org/10.1007/s10712-022-09751-y.\n\n\nWang, Haibo, Xueshuang Gong, Bingbing Wang, Chao Deng, and Qiong Cao.\n2021. “Urban Development Analysis Using Built-up Area Maps Based\non Multiple High-Resolution Satellite Data.” International\nJournal of Applied Earth Observation and Geoinformation 103\n(December): 102500. https://doi.org/10.1016/j.jag.2021.102500."
  }
]