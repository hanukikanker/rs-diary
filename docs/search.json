[
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Viewed from Above: An Learning Diary on Remote Sensing & Earth Observation",
    "section": "About me",
    "text": "About me\n\n\n\n\n\n\nXin chào!\nMy name is The-Huan, although I also go by Huan or Shaun, depending on where in the world you see me.\nHailing from Vietnam, I have lived, studied, and worked in many countries like the United States, Argentina, Denmark, Italy, and Singapore. Most recently, though, I can be found looking at maps and thrift-shopping in London.\nI am an economist by training with a long subsequent stint in the Tech industry. While on the job, I was fascinated by the power of “Big Data” and the cloud. Marrying that with the love for maps and earth observation, I embarked on this journey into Remote Sensing. Come on in.\nI also love talking about (and in) foreign languages I know and am learning. Say hi to me in Vietnamese, English, Spanish, and Italian. Currently working on German and Mandarin.\n\n\nSince you’re still reading…\nGet in touch with me over email or LinkedIn."
  },
  {
    "objectID": "week1.html#summary",
    "href": "week1.html#summary",
    "title": "1  Intro to Remote Sensing",
    "section": "1.1 Summary",
    "text": "1.1 Summary\nRemote Sensing uses satellites, planes, drones, etc., as our aerial eyes, piecing together a portrait of our planet through light and data and revolutionising how we understand and interact with it.\nSensors, well, ‘sense’ Earth in two ways: Some passively listen to sunlight reflected off our planet’s surface (i.e., passive sensors), while others actively send their own signals and capture the echoes (i.e., active sensors). By that definition, the human eye is a type of passive sensors!\n\n\n\nActive vs Passive Remote Sensing (Earth Science Data Systems 2019).\n\n\nThese sensors interpret a fascinating language of light, both invisible and visible, known as the electromagnetic spectrum.\n\n\n\nThe Electromagnetic Spectrum (EMS). Credit: NASA Science\n\n\nElectromagnetic radiation moves as waves as perpendicular electric and magnetic field with a wavelength: λ = c/v where:\n\nλ = wavelength, the distance between two crests\nc = velocity of light 3 x 108 m/sec\nv = frequency, rate of oscillation (full oscillations in a time unit)\n\nDifferent materials reflect unique wavelengths in this spectrum, allowing us to identify them, like decoding DNA!\n\n\n\nWavelength vs. Oscillation\n\n\nBut the information that sensors receive isn’t just about colour. Remote sensing data has its own “resolution” recipe, encompassing:\n\nSpectral: How EMS bands (i.e., the range within the EMS) the sensor can hear, revealing more detail with each additional band.\nSpatial: The size of each pixel in the image, ranging from centimetres to kilometres, offers varying detail levels.\nTemporal: How often the sensor revisits the same area, providing a dynamic view of changes over time.\nRadiometric: The range of brightness levels captured, painting a vibrant and accurate picture.\n\nDepending on the purpose, each sensor is equipped to have better resolution of one type over the other. For example, sensors with a high spatial resolution of 5m (i.e., each pixel is 10x10 on the ground) will have a lower spectral resolution (i.e., capturing only a narrow range of the EMS) (“Remote Sensing, Satellite Imaging Technology | Satellite Imaging Corp” n.d.)"
  },
  {
    "objectID": "week1.html#application",
    "href": "week1.html#application",
    "title": "1  Intro to Remote Sensing",
    "section": "1.2 Application",
    "text": "1.2 Application\nRemote Sensing has many transformative applications in the realm of Urban Analytics. Think of it as an X-ray for cities, revealing hidden patterns and empowering informed decision-making by city planners, urban designers, and public officials to make swift and informed decisions to improve the lives of millions of urbanites.\nHere are some examples of the use of Remote Sensing in urban analytics research\n\nMapping Urban Growth: By tracking changes in the built environment over time, we can identify sprawling suburbs, monitor urban expansion, and plan for infrastructure needs. Recent works have used a diverse source of high-resolution Remote Sensing data (2m) to train a machine-learning model to extract ‘urbanised’ areas more robustly compared to a previous methodology using only medium-resolution images. (Wang et al. 2021)\n\n\n\n\nTransferable built-up area extraction (TBUAE) framework to map urbanised areas\n\n\n\nPredicting Floods with Foresight: Analysing land cover and terrain, it anticipates where water will flow, safeguarding communities from harm while also estimating potential damage. This type of risk assessment is highly relevant not only in climate science and the public sector but also among insurance companies who make use of SAR satellite data (for through-cloud vision) combined with other spatial data sets to assess risk and process claims (Schumann et al. 2023)\n\n\n\n\nRemote sensing data can be used to estimate flood extent and to derive individual risk level damage estimates.\n\n\n\nEnergy Efficiency: Identifying heat island effects and understanding building energy consumption through thermal imaging empowers planners to design sustainable cities and researchers. Remote Sensing data is a key component in Urban Climate Models such as UrbCLIM by VITO, a Belgian research organisation, which aims to create an interactive tool providing high-resolution urban heat maps and predict their evolution under future climate conditions. (“DestinE for Human Heat Stress: ECMWF Use Case to Tackle Urban Heat Islands” n.d.)\n\n\n\n\nLeft: Average 2m air temperature at 23h (moment of max Urban Heat Islands) during all summer months of 1987- 2016. Right: UrbCLIM climate model’s output field showing the average daily maximum urban heat island intensity for Amsterdam\n\n\nLastly, it is worth noting that depending on the objective of the research, different sensors or types of remote sensing data can be used, purely or mixed and matched.\n\n\n\nsd"
  },
  {
    "objectID": "week1.html#reflection",
    "href": "week1.html#reflection",
    "title": "1  Intro to Remote Sensing",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\nMy first foray into the world of Remote Sensing was eye-opening. For the uninitiated, it is easy to assume that remote sensing purely means orthophotography satellite images that one might see using platforms such as Google Maps, as if the only thing sensors do is snap a simple photo of the planet like a phone camera.\nIn reality, it unlocks unseen depths of data beyond mere human perception, not just high-resolution photographs and more spectral signatures, where each pixel holds a universe of information. Building a true-colour composite from all these layers (so that our limited human vision can perceive them) was like seeing the world through a brand-new lens.\nI am particularly excited to get started with Google Earth Engine later on as the primary gateway to access the wealth of remote sensing data and analytics more easily and solve specific problems facing our world today.\n\n\n\n\n“DestinE for Human Heat Stress: ECMWF Use Case to Tackle Urban Heat Islands.” n.d. Accessed January 29, 2024. https://stories.ecmwf.int/destine-for-human-heat-stress-ecmwf-use-case-to-tackle-urban-heat-islands/.\n\n\nEarth Science Data Systems, NASA. 2019. “What Is Remote Sensing? | Earthdata.” Backgrounder. August 23, 2019. https://www.earthdata.nasa.gov/learn/backgrounders/remote-sensing.\n\n\n“Remote Sensing, Satellite Imaging Technology | Satellite Imaging Corp.” n.d. Accessed January 27, 2024. https://www.satimagingcorp.com/services/resources/characterization-of-satellite-remote-sensing-systems/.\n\n\nSchumann, Guy, Laura Giustarini, Angelica Tarpanelli, Ben Jarihani, and Sandro Martinis. 2023. “Flood Modeling and Prediction Using Earth Observation Data.” Surveys in Geophysics 44 (5): 1553–78. https://doi.org/10.1007/s10712-022-09751-y.\n\n\nWang, Haibo, Xueshuang Gong, Bingbing Wang, Chao Deng, and Qiong Cao. 2021. “Urban Development Analysis Using Built-up Area Maps Based on Multiple High-Resolution Satellite Data.” International Journal of Applied Earth Observation and Geoinformation 103 (December): 102500. https://doi.org/10.1016/j.jag.2021.102500."
  },
  {
    "objectID": "week2.html",
    "href": "week2.html",
    "title": "2  A peek into the LiDAR technology",
    "section": "",
    "text": "Having understood how sensors work in theory, we can now shift our view to appreciate how one of them in particular, LiDAR (Light Detection and Ranging), is deployed in practice and can benefit us not only in an emerging sector like Autonomous Vehicles but also in Urban Planning"
  },
  {
    "objectID": "week3.html#summary",
    "href": "week3.html#summary",
    "title": "3  Corrections and Enhancements",
    "section": "3.1 Summary",
    "text": "3.1 Summary\nRaw data from sensors are rarely immediately usable without being corrected for various interferences and effects. Common correction methods are geometric and topographic corrections and radiometric and atmospheric corrections. Finally, enhancement methods help us highlight values of interest that pertain to our research scope.\n\n3.1.1 Radiometric and Atmospheric corrections\nThese processes describe translating raw light data from the sensor into ‘true’ information on the surface’s reflectance property without interference from the light source and the atmosphere.\n\nRadiometric calibration is the conversion from raw Digital Number (raw, no units) to Spectral Radiance via a linear transformation \\(L_λ=Bias+(Gain∗DN)\\). Radiance most often has units of watt/(steradian/square meter)\nAtmospheric correction is the next step:\n\nTOA Radiance-to-Reflectance correction removes the effects of the light source (e.g. the sun) by calibrating radiation going down (irradiance) and up (radiance). TOA Reflectance still affects the atmosphere and the surface material. If irradiance is equal to radiance, we call this hemispheric reflectance.\nTOA-to-BOA Reflectance correction removes the effects of the atmospheric conditions, leaving us with just data on the surface materials. If shadows and directional effects on reflectance have been dealt with, we get what is called true reflectance; if not, it is called apparent reflectance.\n\n\nAtmospheric correction deserves our attention, considering the effect of atmospheric scattering on the final results. Absorption and scattering create the haze, which reduces the contrast and can create the “adjacency effect”, whereby radiance from pixels nearby is mixed into pixels of interest. Atmospheric correction to obtain actual reflectance is not always necessary, for example, for classifying a single image, working on composite images, etc. There are two types of atmospheric correction: -\n\nRelative: normalise intensities of different bands within a single image or from many dates to one date. This can be done via Dark Object Subtraction (DOS) or Pseudo-invariant Features (PIFs).\nAbsolute: change digital brightness values into scaled surface reflectance. We can then compare these scaled surface reflectance values across the planet through atmospheric radiative transfer models (i.e. summer, tropical) or Empirical Line Correction.\n\n\n\n\nExample of LEDAPS atmospheric correction. (a) Top-of-atmosphere (TOA) reflectance composite (bands 3,2,1) for Landsat-7 ETM+ image of San Francisco Bay (July 7, 1999); (b) Surface reflectance composite.\n\n\n\n\n3.1.2 Geometric and topographic corrections\nThese are subsets of Georectification, which gives coordinates to an image and accounts for view angle, topography, wind disturbance, Earth rotation, etc., distorting the resultant image’s geometry.\nTopographic correction corrects the view angle of the image so that it is nadir (i.e., directly top-down). Important concepts to get familiar with for orthorectification are:\n\nSolar azimuth: compass angle of the sun (N =0°) 90° (E) at sunrise and 270° (W) at sunset.\nSolar zenith: angle of local zenith (above the point on the ground) and sun from vertical (90° - elevation)\n\n\n\n\n\n\nGeometric correction effectively ‘grounds’ the images into a georeferenced final product (i.e., with a coordinate). We identify Ground Control Points (GPS) to match known points in the image and a reference dataset. We then model the coordinates to give geometric transformation coefficients (linear regression). It effectively resembles fitting old maps into a digital version.\n\n\n\n\n\n\n\n3.1.3 Joining and Enhancements\nTo join (i.e., ‘mosaicking’), within the overlap area (20-30%), a representative sample is taken, a histogram is extracted from the base image, which is then applied to other images using a histogram matching algorithm to blend the brightness values of the two images (‘feathering’)\nFinally, after all the corrections, there are still many enhancements that can improve or accentuate the visual results depending on the purpose of the research:\n\nContrast enhancement to accentuate reflectance values that are close to each other\nRatio calculation calculates pixel value as a ratio of different bands (e.g. Normalised Burn Ratio)\nFiltering is the use of low-pass filters that average (i.e., smooth) the data or high-pass filters that enhance the variances between features. Filtering is used to perform texture and edge detection.\nPCA transforms multi-spectral data into uncorrelated datasets. Multi-date overlay PCA is a way to detect change efficiently.\nFusion entails fusing images/data from multiple sensors to improve details, enable better classification, or downsample."
  },
  {
    "objectID": "week3.html#application",
    "href": "week3.html#application",
    "title": "3  Corrections and Enhancements",
    "section": "3.2 Application",
    "text": "3.2 Application\nDespite having delved a lot into corrections, it is worth noting that standard remote sensing products now come corrected (Level-2, or “Analysis Ready Data”, ARD). In contrast, products derived from corrected ones are called Level-3. Research that works directly with Level-1 products often seeks to refine the correction methodology to transform them into ARDs (Coluzzi et al. 2018)\nEnhancements made to ARD (i.e. Level-3 products) represent a proliferating field to survey, thanks to its wealth of innovative applications of a single or combinations of techniques, depending on the task at hand. According to recent research that made use of remote sensing data, there seem to be two main umbrella objectives researchers have when considering which enhancement methods to employ:\n\nVisualisation: Visual enhancement essentially accentuates the desired subject vs. other details. Contrast stretching to make images appear brighter is often done without much fanfare but is an essential step in using remote sensing data as an artefact perceptible in print to the human eyes. However, image enhancement using band ratio is widely used to highlight certain objects, with indices including NDVI, SAVI, etc., for Vegetation or NDWI, SWI, etc., for Water and Snow. A combination of indices can also be used to produce a composite ratio that can best visualise the desired study area, which was how Macedo et al. (2018) was able to map the holm oak above-ground biomass over a large area with different atmospheric conditions.\nFeature extraction: Many researchers seek to extract novel datasets from remote sensing data for various purposes, including training machine learning models to do the same (and better) for a larger region or globally. This objective is related to but ultimately distinguished from the above because the output is not a cartographic product but a dataset. The main difficulty when tackling this is to classify accurately (avoid false negatives and positives) while retaining the depth of information in each pixel. Li et al. (2022) proposed a pyramid feature extraction (PFE) to construct multi-scale representations of buildings, in which convolutional neural networks were applied on satellite images already gone through a combination of edge and texture detection, which were then again applied to subsequent output in the workflow."
  },
  {
    "objectID": "week3.html#reflection",
    "href": "week3.html#reflection",
    "title": "3  Corrections and Enhancements",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection\nMy initial knowledge of remote sensing was limited, but this exploration proved insightful and rewarding. The vast amount of information obtainable from raw multi-spectral data is impressive and somewhat little-appreciated outside of the geospatial community.\nTherefore, democratising remote sensing data and technological advancements will empower research on Earth’s surface, surpassing local data collection, which is uneven and ununified by nature. Instead, remote sensing can be performed globally if the correct adjustments for the atmosphere and enhancements to fit the objectives are made.\nHowever, while this is an untapped data source, there are two hurdles to overcome before they may be fully utilised.\n\nTechnical competence: Remote sensing is jargon-filled, with each discipline adopting its best practice to harness the data, making the barrier to entry higher than that of other types of data analysis work. Standardisation of industry-agnostic workflow and training may be vital to upskilling geospatial analysts to work more natively with remote sensing data.\nHigh-resolution EO data are mostly not free and reserved for governmental/military use. High-quality EO data (multispectral, high spatial resolution, frequent, etc.) also depends on localities. Sophisticated enhancement techniques may help bridge the gap by fusing existing datasets.\n\n\n\n\n\nColuzzi, Rosa, Vito Imbrenda, Maria Lanfredi, and Tiziana Simoniello. 2018. “A First Assessment of the Sentinel-2 Level 1-C Cloud Mask Product to Support Informed Surface Analyses.” Remote Sensing of Environment 217 (November): 426–43. https://doi.org/10.1016/j.rse.2018.08.009.\n\n\nLi, Wangbin, Kaimin Sun, Hepeng Zhao, Wenzhuo Li, Jinjiang Wei, and Song Gao. 2022. “Extracting Buildings from High-Resolution Remote Sensing Images by Deep ConvNets Equipped with Structural-Cue-Guided Feature Alignment.” International Journal of Applied Earth Observation and Geoinformation 113 (September): 102970. https://doi.org/10.1016/j.jag.2022.102970.\n\n\nMacedo, Fabrício L., Adélia M. O. Sousa, Ana Cristina Gonçalves, José R. Marques da Silva, Paulo A. Mesquita, and Ricardo A. F. Rodrigues. 2018. “Above-Ground Biomass Estimation for Quercus Rotundifolia Using Vegetation Indices Derived from High Spatial Resolution Satellite Images.” European Journal of Remote Sensing 51 (1): 932–44. https://doi.org/10.1080/22797254.2018.1521250."
  },
  {
    "objectID": "week4.html#summary",
    "href": "week4.html#summary",
    "title": "4  In practice: Tracking Soil Organic Carbon",
    "section": "4.1 Summary",
    "text": "4.1 Summary\nThe amount of carbon in the atmosphere has increased by 30% in the past two centuries from high levels of fossil fuel combustion and deforestation, leading to rising global temperatures.\nAmong many well-known strategies to reduce our carbon footprints, such as developing energy-efficient fuels and reducing greenhouse gas emissions, there are also strategies such as soil carbon sequestration, tree planting, and ocean carbon sequestration.\nHow carbon is captured within soil:\n\nPlants capture carbon through photosynthesis, and some remain as plant tissue. Dead plant tissue enters the soil as litter and decomposes.\nDecomposition creates soil organic matter (SOM), the main carbon stored in soil. SOM can hold carbon for a long time or release it back into the atmosphere.\nVarious factors like climate, vegetation, texture, and drainage influence carbon storage.\n\nDirect human intervention (on-farm activities) could also enhance soil carbon sequestration.\n\n\n\nOn-farm activities to enhance soil carbon sequestration (Barbato and Strong 2023)\n\n\nGlobally, there are policies in place to incentivise farmers to undertake on-farm activities that sequester soil carbon.\nFor example, in California’s Cap-and-Trade Program, the private sector can buy carbon credits for their offset targets. Downstream, rice farmers, for instance, may volunteer to implement one of three methods included in the protocol to sequester carbon: dry seeding, early drainage, or alternative wetting or drying at the price per ton CO2e was $7. (“Carbon Farming: Opportunities for Agriculture and Farmers to Gain From Decarbonization” n.d.)\nAdoption challenges for these markets include (1) ensuring accurate carbon measurement and (2) verifying “additionality” - that carbon sequestration wouldn’t have happened without the market incentive. Between the two main challenges, Remote Sensing technology has the most potential in tackling the first at scale, rather than relying on local field work that is labour-intensive and limited by nature, albeit more precise."
  },
  {
    "objectID": "week4.html#application",
    "href": "week4.html#application",
    "title": "4  In practice: Tracking Soil Organic Carbon",
    "section": "4.2 Application",
    "text": "4.2 Application\n\n4.2.1 From academia…\nThere has only been a recently increasing academic focus on the application of Remote Sensing and Earth Observation technologies to map and Monitor Soil Organic Carbon, with much credit to the free and open availability of satellite imagery, such as that of Sentinel missions, facilitated by the Copernicus program. Here are some notable works in this realm and their significance to the state-of-the-art:\nTziolas et al. (2021) is a formative article that provided insights into the usage of AI techniques and other data pipeline rigour and innovations to increase resolutions and better harness EO data. In the article, they aptly pointed out that ‘an EO data-driven approach should be prioritized if progress towards the soil policy and economic strategic goals is to be accelerated’ which reveals the current disconnect between academia and actual adoption of this technology, perhaps due to non-standardised SOC monitoring methodology and a lack of focus on the end-user experience.\nIndeed, a further review of current literature showcases the vast complexity that analysts must contend with in order to reliably measure SOC. For example, van Wesemael et al. (2023) highlighted the need for bare soil conditions and recommended temporal mosaicking to remove irrelevant spectral variations to arrive at a robust SOC prediction model. Upon that conclusion, other researchers delved deeper into diverging techniques to extract bare soil pixels from Sentinel and Landsat data.\n\nUrbina-Salazar et al. (2021) showed that Sentinel 1/2 soil moisture products were helpful in extracting bare and dry soil pixels\nZepp et al. (2021) used multitemporal (30+ years) satellite images as an alternative to retrieve exposed soils from a soil reflectance composite\nDvorakova, Heiden, and van Wesemael (2021) instead focused on a particular period based on a time series of normalised difference vegetation index (NDVI), and concluded that the best SOC prediction could already be achieved for this particular period by using a strict normalised burn ratio 2 (NBR2) threshold.\n\n\n\n\nUsing Sentinel-2 Images for Soil Organic Carbon Content Mapping in Croplands (Urbina-Salazar et al. 2021)\n\n\nOther works focused on improving the prediction model further with higher spatial resolution input data to correspond with average field size (Möller et al. 2022), and techniques to overcome vegetation cover (Heiden et al. 2022)\nOn the other hand, depth matters when it comes to SOC mapping (“Depth Matters for Soil Carbon Accounting – CarbonPlan” n.d.), with best practices requiring soil sampling that encompasses the full crop-rooting zone of a minimum of 30 cm. Therefore, 3D mapping is another frontier where many innovations have emerged. The pseudo-3D technique is the building of separate models for individual depth intervals, whose interval predictions are integrated to get the full profile of SOC.\n\n\n4.2.2 …to the public domain\nThe first Global Soil Organic Matter map was launched on World Soil Day in 2017), by the United Nations Food and Agriculture Organisation (FAO). The map is produced on a common grid for 100 countries, allowing scientists and policymakers to identify degraded areas and explore the potential for soils to capture soil carbon. The map also helps set restoration targets, support the greenhouse gas emission reporting under the United Nations Framework Convention on Climate Change, and make evidence-based decisions to mitigate and adapt. (“Hutton Soil Scientists Help Develop First Global Soil Organic Carbon Map | The James Hutton Institute” n.d.).\n\n\n\nScreenshot of the GSOCMap. Left: Relative Soil Organic Carbon Sequestration Rates. Right: Projected Soil Organic Carbon Stocks\n\n\n\n\n\nStructure of the Implementation process for the Global Soil Partnership (GSP)"
  },
  {
    "objectID": "week4.html#reflection",
    "href": "week4.html#reflection",
    "title": "4  In practice: Tracking Soil Organic Carbon",
    "section": "4.3 Reflection",
    "text": "4.3 Reflection\nA precise and reliable global view of SOC is needed under different UN conventions, such as the UN Convention on Climate Change and Desertification (UNCCD) and the Sustainable Development Goals (SDG). At the national level, such data can be used as reference soil carbon stocks, with the aim of refining national greenhouse gas inventories and assessing the sensitivity of soils to degradation and climate change.\nThat said, scant evidence has been found to show further applications using the GSOC map, or Level-3 EO SOC mapping products, either at the national level at finer spatial resolution or by the private or public sector, as an active component to incentivise stakeholders to take action. In most cases, what GSP was able to produce was a series of guidelines for national stakeholders without imposing shared deliverables and timelines. Instead, the original GSOC maps are used to determine the status quo and rally political interest at the national level and do not possess the spatial resolution needed to make them effective monitoring tools.\nPossible reasons may lie in technical competency to effectively produce national soil maps with the right open data policy to allow the public and other innovators to tap into this data.\n\n\n\n\nBarbato, Clare T., and Aaron L. Strong. 2023. “Farmer Perspectives on Carbon Markets Incentivizing Agricultural Soil Carbon Sequestration.” Npj Climate Action 2 (1, 1): 1–9. https://doi.org/10.1038/s44168-023-00055-4.\n\n\n“Carbon Farming: Opportunities for Agriculture and Farmers to Gain From Decarbonization.” n.d. Accessed February 15, 2024. https://www.spglobal.com/esg/insights/topics/carbon-farming-opportunities-for-agriculture-and-farmers-to-gain-from-decarbonization.\n\n\n“Depth Matters for Soil Carbon Accounting – CarbonPlan.” n.d. Accessed February 15, 2024. https://carbonplan.org/research/soil-depth-sampling.\n\n\nDvorakova, Klara, Uta Heiden, and Bas van Wesemael. 2021. “Sentinel-2 Exposed Soil Composite for Soil Organic Carbon Prediction.” Remote Sensing 13 (9, 9): 1791. https://doi.org/10.3390/rs13091791.\n\n\nHeiden, Uta, Pablo d’Angelo, Peter Schwind, Paul Karlshöfer, Rupert Müller, Simone Zepp, Martin Wiesmeier, and Peter Reinartz. 2022. “Soil Reflectance Composites—Improved Thresholding and Performance Evaluation.” Remote Sensing 14 (18, 18): 4526. https://doi.org/10.3390/rs14184526.\n\n\n“Hutton Soil Scientists Help Develop First Global Soil Organic Carbon Map | The James Hutton Institute.” n.d. Accessed February 15, 2024. https://www.hutton.ac.uk/news/hutton-soil-scientists-help-develop-first-global-soil-organic-carbon-map.\n\n\nMöller, Markus, Simone Zepp, Martin Wiesmeier, Heike Gerighausen, and Uta Heiden. 2022. “Scale-Specific Prediction of Topsoil Organic Carbon Contents Using Terrain Attributes and SCMaP Soil Reflectance Composites.” Remote Sensing 14 (10, 10): 2295. https://doi.org/10.3390/rs14102295.\n\n\nTziolas, Nikolaos, Nikolaos Tsakiridis, Sabine Chabrillat, José A. M. Demattê, Eyal Ben-Dor, Asa Gholizadeh, George Zalidis, and Bas van Wesemael. 2021. “Earth Observation Data-Driven Cropland Soil Monitoring: A Review.” Remote Sensing 13 (21, 21): 4439. https://doi.org/10.3390/rs13214439.\n\n\nUrbina-Salazar, Diego, Emmanuelle Vaudour, Nicolas Baghdadi, Eric Ceschia, Anne C. Richer-de-Forges, Sébastien Lehmann, and Dominique Arrouays. 2021. “Using Sentinel-2 Images for Soil Organic Carbon Content Mapping in Croplands of Southwestern France. The Usefulness of Sentinel-1/2 Derived Moisture Maps and Mismatches Between Sentinel Images and Sampling Dates.” Remote Sensing 13 (24, 24): 5115. https://doi.org/10.3390/rs13245115.\n\n\nWesemael, Bas van, Sabine Chabrillat, Adrian Sanz Dias, Michael Berger, and Zoltan Szantoi. 2023. “Remote Sensing for Soil Organic Carbon Mapping and Monitoring.” Remote Sensing 15 (14, 14): 3464. https://doi.org/10.3390/rs15143464.\n\n\nZepp, Simone, Uta Heiden, Martin Bachmann, Martin Wiesmeier, Michael Steininger, and Bas van Wesemael. 2021. “Estimation of Soil Organic Carbon Contents in Croplands of Bavaria from SCMaP Soil Reflectance Composites.” Remote Sensing 13 (16, 16): 3141. https://doi.org/10.3390/rs13163141."
  },
  {
    "objectID": "week6.html#summary",
    "href": "week6.html#summary",
    "title": "5  Discovering Google Earth Engine",
    "section": "5.1 Summary",
    "text": "5.1 Summary\nGoogle Earth Engine is a cloud-based geospatial processing platform with incredible capabilities for environmental monitoring and analysis. It is well-appreciated for its ability to analyse large-scale remote sensing data within seconds without relying on local computers’ storage and memory. Data processing in GEE is facilitated by utilizing JavaScript within the platform’s web interface (or Code Editor) or with Google Colab notebook, employing Python API (combined with the visualisation power of the geemap library) for a more familiar data science computing environment.\n\n\n\nSource: (“Google Earth Engine Comparison Between Python and JavaScript | Bikesh Bade” n.d.)\n\n\nIn summary, the Google Earth Engine (GEE) platform provides users with:\n\nExtensive data access: Explore petabytes of free, ready-to-use remote sensing imagery and other products, not to mention many more contributed by the Awesome GEE Community.\nPowerful processing: Leverage Google’s computational infrastructure for high-speed parallel processing and advanced machine learning algorithms.\nScale and Projection automation: GEE automatically populates the appropriate pixels depending on the scale of the analysis and resamples if needed when producing the output. It also automatically converts all data into the Mercator projection when displayed.\nFlexible development environment: Utilize a library of Application Programming Interfaces (APIs) with coding support for popular languages like JavaScript and Python. However, the GEE Python API currently lacks robust documentation and comprehensive interactive visualization capabilities compared to the JavaScript API. The geemap Python package bridges this gap. Built upon open-source libraries like earthengine-api, folium, ipyleaflet, and ipywidgets, it empowers users to conduct interactive analysis and visualization of Earth Engine datasets within a Jupyter environment through intuitive coding.\n\nHere are a few things one can accomplish with Google Earth Engine:\n\nJoins with ee.Filter() and join.apply()\nZonal statistics with reduceRegion() or reduceNeighborhood().\nLinear regression with linearFit() and linearRegression()\nMachine Learning including supervised/unsupervised classification and deep learning with TensorFlow to be covered in subsequent weeks\nVisualisation and web geospatial applications with GEE data. Some prominent applications will be discussed below"
  },
  {
    "objectID": "week6.html#application",
    "href": "week6.html#application",
    "title": "5  Discovering Google Earth Engine",
    "section": "5.2 Application",
    "text": "5.2 Application\nWhile Google Earth Engine’s potential applications have not fundamentally changed the field of remote sensing analysis, its impact lies in revolutionizing the accessibility and scale of such work. Previously, researchers faced significant hurdles in downloading massive datasets and processing cumbersome raster files. Earth Engine eliminates these obstacles, allowing for faster, more efficient, and ultimately more creative analysis of potentially planet-level scale.\nAn investigation into the literature that used GEE in its methodology revealed that the most popular applications are Water Resources, Land Use, Agriculture, and Natural Hazards (Pérez-Cutillas et al. 2023). It is interesting to see that climate change and urban spatial research seem to be trailing behind other fields in employing GEE, but it is possible that the author’s classification obscures the fact that a piece of research could belong to two or more application areas (e.g. flood risk analysis could be considered Natural Hazards or Climate Change).\n\n\n\nDistribution of cases by application area. Applications: Agriculture (A); Climate change (CC); Natural hazards (NH); Forestry (F); Water Resources (WR); Soils (S); Urban (U); Land use (LU). (Pérez-Cutillas et al. 2023)\n\n\nHere are some application areas of GEE, along with specific research examples.\n\nFlood risk assessment and mitigation: Example: “Urban flood risk assessment using Sentinel-1 on the Google Earth engine: A case study in Thai Nguyen city, Vietnam” by Mai Sy et al. (2023)\nLand Use/Land Cover and Change Detection: Example: “Characterizing annual dynamics of urban form at the horizontal and vertical dimensions using long-term Landsat time series data” by Wang et al. (2023)\nAgriculture and Food Security: Example: “Drought Monitoring Using Landsat Derived Indices and Google Earth Engine Platform: A Case Study from Al-Lith Watershed, Kingdom of Saudi Arabia” by Ejaz et al. (2023)\nClimate Disaster Monitoring: Example: “A google earth engine approach for anthropogenic forest fire assessment with remote sensing data in Rema-Kalenga wildlife sanctuary, Bangladesh” by Mohammed et al. (2023)\nUrban Heat Island Monitoring: Example: “Monitoring the Impact of Land Cover Change on Surface Urban Heat Island through Google Earth Engine: Proposal of a Global Methodology, First Applications and Problems” by Ravanelli et al. (2018)\n\nIt is worth noting that in much of the research literature I have encountered, GEE is often highlighted and used prominently for applications in regions of the Global South. This is possibly due to various reasons:\n\nLack of reliable up-to-date geospatial datasets for urgent local issues (flood, drought, deforestation, etc.), motivating the public sector and researchers to be resourceful by using freely available Remote Sensing data.\nLow reliance on the old geospatial tech stack, thus can nimbly adopt GEE without much issue.\nConsequently, low/No computing costs compared to traditional methods.\n\nIt would be interesting to see how the developing world will pioneer more ground-breaking research methodologies leveraging GEE."
  },
  {
    "objectID": "week6.html#reflection",
    "href": "week6.html#reflection",
    "title": "5  Discovering Google Earth Engine",
    "section": "5.3 Reflection",
    "text": "5.3 Reflection\nWorking with Google Earth Engine (GEE) was a revelation. Compared to traditional methods like SNAP or accessing data from Copernicus Open Access Hub, GEE offered exceptional efficiency in acquiring satellite imagery.\nThis introduction to GEE came with a crash course in JavaScript programming, which I had not been exposed to before. There were also concepts such as client vs server-side functions that are particular to GEE that one has to wrestle with when programming. In the future, I will explore how to replicate what will be introduced in both CASA0023 and CASA0025 regarding Google Earth Engine in a Python environment, either via the API or via geemap library, to integrate it with other data science workflows. I’m particularly interested in exploring GEE’s capabilities for studying and predicting temporal changes, specifically aiming to showcase the impact of global warming, flooding, droughts, etc. Therefore, leveraging GEE and being able to tap into Python Machine Learning libraries will be a powerful combination.\n\n\n\n\nEjaz, Nuaman, Jarbou Bahrawi, Khalid Mohammed Alghamdi, Khalil Ur Rahman, and Songhao Shang. 2023. “Drought Monitoring Using Landsat Derived Indices and Google Earth Engine Platform: A Case Study from Al-Lith Watershed, Kingdom of Saudi Arabia.” Remote Sensing 15 (4, 4): 984. https://doi.org/10.3390/rs15040984.\n\n\n“Google Earth Engine Comparison Between Python and JavaScript | Bikesh Bade.” n.d. Accessed March 7, 2024. https://bikeshbade.com.np/tutorials/Detail/?title=Google%20earth%20engine%20Detail%20comparison%20between%20Python%20and%20JavaScript&code=9.\n\n\nMai Sy, Hung, Chinh Luu, Quynh Duy Bui, Hang Ha, and Dinh Quoc Nguyen. 2023. “Urban Flood Risk Assessment Using Sentinel-1 on the Google Earth Engine: A Case Study in Thai Nguyen City, Vietnam.” Remote Sensing Applications: Society and Environment 31 (August): 100987. https://doi.org/10.1016/j.rsase.2023.100987.\n\n\nMohammed, Ariful Khan, Angana Kuri, Sohag Ahammed, Kazi Al Muqtadir Abir, and Mohammed A. S. Arfin-Khan. 2023. “A Google Earth Engine Approach for Anthropogenic Forest Fire Assessment with Remote Sensing Data in Rema-Kalenga Wildlife Sanctuary, Bangladesh.” Geology, Ecology, and Landscapes 0 (0): 1–22. https://doi.org/10.1080/24749508.2023.2165297.\n\n\nPérez-Cutillas, Pedro, Alberto Pérez-Navarro, Carmelo Conesa-García, Demetrio Antonio Zema, and Jesús Pilar Amado-Álvarez. 2023. “What Is Going on Within Google Earth Engine? A Systematic Review and Meta-Analysis.” Remote Sensing Applications: Society and Environment 29 (January): 100907. https://doi.org/10.1016/j.rsase.2022.100907.\n\n\nRavanelli, Roberta, Andrea Nascetti, Raffaella Valeria Cirigliano, Clarissa Di Rico, Giovanni Leuzzi, Paolo Monti, and Mattia Crespi. 2018. “Monitoring the Impact of Land Cover Change on Surface Urban Heat Island Through Google Earth Engine: Proposal of a Global Methodology, First Applications and Problems.” Remote Sensing 10 (9, 9): 1488. https://doi.org/10.3390/rs10091488.\n\n\nWang, Yixuan, Xuecao Li, Peiyi Yin, Guojiang Yu, Wenting Cao, Jinxiu Liu, Lin Pei, et al. 2023. “Characterizing Annual Dynamics of Urban Form at the Horizontal and Vertical Dimensions Using Long-Term Landsat Time Series Data.” ISPRS Journal of Photogrammetry and Remote Sensing 203 (September): 199–210. https://doi.org/10.1016/j.isprsjprs.2023.07.025."
  },
  {
    "objectID": "week7.html#summary",
    "href": "week7.html#summary",
    "title": "6  Classification I: Making sense of the pixel",
    "section": "6.1 Summary",
    "text": "6.1 Summary\nWe continue the journey in Remote Sensing and Earth Observation analysis by exploring various ways to derive insights from this data and imagery using Machine Learning. These insights may come from Classification and Regression Trees (CART) to Random Forests and more cutting-edge Object-based Image Analysis (OBIA) methods. Many of these methods can be deployable natively within Google Earth Engine, further enhancing the platform’s attractiveness to geospatial data scientists.\nFor this entry, we will examine a few key supervised and unsupervised classification methods specific to remote sensing to classify every pixel in the image into one of the pre-defined categories. Then, in the next entry, we will discuss advanced image classification methods such as OBIA and model validation and calibrationcla#.\n\n6.1.1 Unsupervised classification\nIdentifying land cover classes unknown beforehand by clustering and labelling based on the spectral info it has. It is most suitable for exploratory analysis when examining an unfamiliar feature space. Three popular algorithms are K-mean, DGSCAN clustering and ISODATA.\n\nK-means: centroid-based clustering method. It groups pixels/objects into pre-defined numbers of groups based on their spectral properties and predetermined distance metrics.\nDBSCAN: a density-based spatial clustering method. It helps identify clusters of objects or features in an image where the number of classes is unknown.\nISODATA: stands for “Iterative Self-Organizing Data Analysis Technique.” It is an iterative method for clustering data elements into different classes while allowing the merging of too similar clusters or elongated clusters in the feature space.\n\n\n\n\nAn example of LULC classification using ISODATA clustering. Clusters are then to be labelled according to cluster properties. (Kganyago and Sibandze 2014)\n\n\n\n\n6.1.2 Supervised classification\nTeaching classifiers to learn to recognise patterns so that they can place labels on new data. It could be pixels, objects, or textures. My impression is that supervised algorithms are more popular than unsupervised ones in the realm of remote sensing for Land Use and Land Cover classification because of their deterministic nature.\n\nClassification and Regression Tree (CART): Takes and predicts discrete values by putting all values through a series of splits with the goal of minimising GINI impurity within each leaf node until a stopping criteria (min members, max depth).\nRandom Forest: basically an ensemble of CART (hence, a forest) from various bootstrap samples of the data (hence, random). Data are classified through a majority decision from all trees.\nMaximum Likelihood: assumes that each class in each band are normally distributed and calculates the probability that a given pixel belongs to a specific class. We can set a prior probability threshold to support the classification.\nSupport Vector Machine: A linear binary classifier. Essentially, this method tries to construct a linear divider (or ‘support vector’) in the feature space to separate the data points into different classes by maximising the margin between the two classes or minimising wrongly classified points (‘soft margin’)\n\n\n\n\nExample of tuning SVM to improve accuracy in multi-temporal remote sensing imagery classification (Guo, Jia, and Paull 2017)\n\n\nIt is worth noting that classification in remote sensing requires some extra considerations. Besides the fact that there are trade-offs between interpretability and accuracy, just like in other realms, analysts also have to decide whether to perform fuzzy classification (partial membership in multiple classes) and whether we are classifying pixels or objects."
  },
  {
    "objectID": "week7.html#application",
    "href": "week7.html#application",
    "title": "6  Classification I: Making sense of the pixel",
    "section": "6.2 Application",
    "text": "6.2 Application\nAs seen from the previous section, the most popular use case of classification seems to be land use and cover (LULC) classification. The purpose is to quantify urban or rural changes towards scientific or policy-making ends. Methodology varies from one locale to another depending on the nature of the remote sensing data product available, endogenous factors like vegetation species, urban roofs, etc.\n\nFor example, this study by Talukdar et al. (2020) compared six different algorithms for classifying LULC from satellite imagery (supervised classification). These are Support Vector Machine, Minimum Distance, Random Forest, Fuzzy ARTMAP, Artificial Neural Networks, and Spectral Angle Mapper. Among those tested, although Artificial Neural Networks (ANN) was a strong performer, the study concludes that Random Forest is the best option for this particular environment (West Bengal, India), having achieved the highest accuracy (0.89).\n\n\n\n\nValidation of LULC of different classifiers with satellite data-derived indices (NDVI, NDWI, NDBI). Source: Talukdar et al. (2020)\n\n\n\nAnother study by Jansen and Di Gregorio (2003) on Kiambu, Kenya, delved into the idea that supervised classification requires a robust set of classes to train any algorithm that varies depending on the researchers’ needs, and that LU/LC are inherently linked but different concepts. This research proposes a method to turn a land cover map into a more useful land use map. Land cover describes physical features (forest, water, etc.) while land use describes how humans interact with those features (agriculture, urban areas, etc.). The final product is a detailed land use map where users can customize how the data is grouped based on their needs.\n\n\n\n\nLand-use data acquisition procedure including the land cover interpretation. Source: Jansen and Di Gregorio (2003)\n\n\nLULC classification in the urban analytics context, when combined with a change detection algorithm, can be used to analyse urban sprawl, urban heat islands, urban physical change, urban climate modelling, and other socio-ecological studies."
  },
  {
    "objectID": "week7.html#reflection",
    "href": "week7.html#reflection",
    "title": "6  Classification I: Making sense of the pixel",
    "section": "6.3 Reflection",
    "text": "6.3 Reflection\nUpon reaching this part of the module, a lot of fragmented pieces of knowledge from different modules I had gathered started to come together nicely. At the start of the module, we learned about the science of remote sensing and the different types of RS data products, which may or may not have gone through corrections and enhancements. However, the final product we receive is only the beginning and opens up a plethora of possibilities for how to bring them into actual academic and real-world applications, such as LULC classification! At the same time, we have built up knowledge about Machine Learning algorithms in a generic context and are now able to see them in action with remote sensing data.\nFinally, all this would not have been possible if we did not know how to pose the right research question pertaining to urban spatial properties that can only be solved with RS and not others, e.g. LULC change detection. Besides much theory, being able to apply it seamlessly via Google Earth Engine has made what had seemed inaccessible thus far accessible to the general public. With the advancement in the field, RS data can truly level the playing field, especially for data-poor localities (i.e., with few official spatial datasets)\nOne remaining question I have before discussing the next few topics is about imputation. Many of the possibilities offered by Remote Sensing data rely on relatively high spatial, temporal, and/or spectral resolution. For example, pixel-based classification methods are especially reliant on spatial resolution, and some change detection goals need high temporal frequency, even hourly. Are there ways to make the most out of free/low-cost RS data with lower than optimal resolution while not sacrificing accuracy?\n\n\n\n\nGuo, Yiqing, Xiuping Jia, and David Paull. 2017. “A Domain-Transfer Support Vector Machine for Multi-Temporal Remote Sensing Imagery Classification.” 2017 IEEE International Geoscience and Remote Sensing Symposium (IGARSS), July, 2215–18. https://doi.org/10.1109/IGARSS.2017.8127428.\n\n\nJansen, Louisa J. M, and Antonio Di Gregorio. 2003. “Land-Use Data Collection Using the ‘Land Cover Classification System’: Results from a Case Study in Kenya.” Land Use Policy 20 (2): 131–48. https://doi.org/10.1016/S0264-8377(02)00081-9.\n\n\nKganyago, Mahlatse, and Phila Sibandze. 2014. Evaluating the Performance of Pixel-based Vs. Object-based Classifiers for Extracting High Resolution Land Cover Product from SPOT 6 Imagery (Non-peer Reviewed). https://doi.org/10.13140/2.1.4177.6002.\n\n\nTalukdar, Swapan, Pankaj Singha, Susanta Mahato, Shahfahad, Swades Pal, Yuei-An Liou, and Atiqur Rahman. 2020. “Land-Use Land-Cover Classification by Machine Learning Classifiers for Satellite Observations—A Review.” Remote Sensing 12 (7, 7): 1135. https://doi.org/10.3390/rs12071135."
  },
  {
    "objectID": "week8.html#summary",
    "href": "week8.html#summary",
    "title": "7  Classification II: Beyond the pixels and Accuracy",
    "section": "7.1 Summary",
    "text": "7.1 Summary\nSo far, we have examined various methods to classify pixels into different classes via supervised and unsupervised learning. However, for remote sensing data (or geospatial data in general), reality doesn’t stop at the single-pixel level. This week, we will examine two image classification methods before assessing the accuracy of a spatial classification model.\n\n7.1.1 Advanced image classification methods\n\nObject-Based Image Analysis (aka OBIA or Super-pixel Analysis): before being fed into a classification algorithm, pixels are first clustered into super-pixels. OBIA allows researchers to extract meaningful objects from imagery by leveraging spectral information (colour) and spatial context (how pixels are arranged). There are many clustering methods:\n\nSLIC (Simple Linear Iterative Clustering) clusters pixels into superpixels based on the spatial distance among the pixels and colour difference.\nSupercells clusters using any dissimilarity measure not just Euclidean distance and based on LAB colour space\n\n\n\n\n\nAn example of the classification pipeline that involves OBIA (Blaschke et al. 2014)\n\n\n\nSub-pixel Analysis: In similar veins but in the opposite direction, sometimes pixels contain more than one class (especially low-resolution data). Therefore, our analysis may benefit from assigning a pixel partial membership with a certain weight for each class. Sub-pixel analysis can be standardised to make the analysis in certain contexts more efficient and interpretable, such as leveraging the V-I-S model in urban areas (three end members). On the other hand, it can also be expanded in complexity in the form of Multiple Endmember Spectral Mixture Analysis (MESMA) in which many possible mixture models are examined to produce the best fit\n\n\n\n\nAn example of MESMA in action classifying urban land cover (Degerickx, Roberts, and Somers 2019)\n\n\n\n\n7.1.2 Accuracy Assessment\nThere are many ways to measure the accuracy of an ML model. However, some of the most popular ones frequently employed in literature are user’s accuracy, producer’s accuracy, overall accuracy and F1 score\n\n\n\nSource: (Tutorials n.d.)\n\n\n\nProducer’s accuracy (recall) is defined as the fraction of correctly classified pixels TP compared to ground truth data TP+FN, loosely how many % of the actual values are correctly classified. Error of omission is the inverse of this.\nUser’s accuracy (precision) is defined as the fraction of correctly classified pixels TP relative to all others classified as a particular land cover TP+FP, loosely how many % of the predicted values are correctly classified. Error of commission is the inverse of this.\nOverall accuracy that represents the combined fraction of correctly classified pixels TP+TN across all land cover types TP+TN+FP+FN\nThe F1 score combines PA and UA into one measure: TP / TP + 0.5*(FP+FN).it penalises FP and FN, while overall accuracy also awards TN.\n\nHowever, optimising F1-score only may obscure TN, while optimising OA alone conflates TP and TN. In cases where TN is as important (e.g., when there are many classes of different distributions and correctly predicting negatives of each class is necessary), we could use Area under the Receiver Operating Characteristic Curve (AUROC), which essentially both maximises TP and minimises FP (thus increasing the TN rate). AUROC can be used to compare different models more effectively.\n\n\n\nSource: (“Measuring Performance: AUC (AUROC) – Glass Box” n.d.)\n\n\n\n\n7.1.3 Spatial Autocorrelation in Machine Learning\nOne interesting detail from this topic is that machine learning on spatial systems has a distinct issue of spatial correlation that may not be present in other applications. One consequence of this is that the train/test split needs extra care in case the data in each are close to each other spatially, thus causing data leakage and affecting the accuracy assessment of the model (i.e., making it unfair)\n\nOne method is to perform ML using OBIA instead of pixels since the process of creating superpixels already reduces spatial correlation (pixels close to each other are already bundled into one bigger object different from the one adjacent to it)\nAnother method to apply is spatial cross-validation, where Train/Test splitting is done in a spatially-aware manner, establishing a required boundary between them to minimise the leakage mentioned above. This could also be done using k-means clustering or using SVM.\n\n\n\n\nSchematic of hyperparameter tuning and performance estimation levels in Cross-Validation. (Schratz et al. 2019)"
  },
  {
    "objectID": "week8.html#application",
    "href": "week8.html#application",
    "title": "7  Classification II: Beyond the pixels and Accuracy",
    "section": "7.2 Application",
    "text": "7.2 Application\nLooking beyond the pixels enables many more additional applications than just LULC classification. If pixel-based classification can help detect urban vs rural land use, subpixel analysis and OBIA can map impervious surfaces (Shao et al. 2023), granular green spaces such as lawns (Zhou, Troy, and Grove 2008), and building footprint extraction and classification (Zhang, Han, and Bogus 2020), etc.\nLet’s take a closer look at Zhang, Han, and Bogus (2020)’s methodology, as it shows many possibilities made available with OBIA to be fused with other techniques to extract beyond what can be seen, i.e., not just building footprint but also height:\n\nLiDAR was used to create two 3D models of the area: one representing the surface with everything on it (DSM) and another representing the bare ground (DTM). The ground model was then subtracted from the surface model to get a difference model (DHM) showing the object’s height above the ground.\n\n\n\n\nThree generated models (a) DSM, (b) DTM, and (c) DHM. Source: (Zhang, Han, and Bogus 2020)\n\n\n\nOBIA applied to first provide the raw set of footprints which went through a series of additional corrections. This was fused with the DHM to identify potential building footprints and heights. This is validated finally with ground-truth data.\n\n\n\n\nFiltering final results to get building footprints (a) Before size selection, (b) After size selection, (c) Polygon dissolving, (d) Eliminating small parts. Source: Zhang, Han, and Bogus (2020)\n\n\nFinally, on the topic of accuracy assessment, this paper terms PA as Completeness, UA as Correctness, and a modified F1-score as Quality–the denominator is TP+FN+FP instead of TP+0.5*(FN+FP). In addition, the similarity of building shapes extracted using OBIA was assessed using metrics like perimeter ratio r(P) and area ratio r(A), which tell us how closely the shapes of the detected buildings resemble the actual buildings on the ground. The closer these ratios are to 0, the more similar they are."
  },
  {
    "objectID": "week8.html#reflection",
    "href": "week8.html#reflection",
    "title": "7  Classification II: Beyond the pixels and Accuracy",
    "section": "7.3 Reflection",
    "text": "7.3 Reflection\nThis week’s investigation into advanced classification, including OBIA, has been eye-opening. It operates upon concepts that humans are most attuned to: We perceive ‘stuff’ as objects instead of a bunch of pixels, and computer vision can now replicate that sort of world perception. It amazes me somewhat how we could bridge the gap between pixel and object detection in remote sensing data using seemingly logical and simplistic assumptions such as colour consistency and continuity to great accuracy results.\nSpeaking of accuracy and many gaps present in machine learning on RS data, although it poses more questions than answers, I was intrigued by the level of critical subjective judgment required to be a successful geospatial analyst. For example, what accuracy metric to use for each application has differed much from one academic circle to another, and has no standardised approach. That said, it was reassuring to hear (from Andy’s_ that AUROC has been more widely supported to be an effective accuracy metric based. On a conceptual level, it rightly puts the focus on the users of the insights rather than the data scientists while avoiding a skewed assessment produced by the User’s accuracy metric, which is unreliable for high variances among classes (often the case for RS data). I am interested in finding out newer techniques developed to better perform object detection and overcome data quality or ground truth limitations (i.e., low resolution, unknown classes, etc.).\n\n\n\n\nBlaschke, Thomas, Geoffrey J. Hay, Maggi Kelly, Stefan Lang, Peter Hofmann, Elisabeth Addink, Raul Queiroz Feitosa, et al. 2014. “Geographic Object-Based Image Analysis – Towards a New Paradigm.” ISPRS Journal of Photogrammetry and Remote Sensing 87 (January): 180–91. https://doi.org/10.1016/j.isprsjprs.2013.09.014.\n\n\nDegerickx, J., D. A. Roberts, and B. Somers. 2019. “Enhancing the Performance of Multiple Endmember Spectral Mixture Analysis (MESMA) for Urban Land Cover Mapping Using Airborne Lidar Data and Band Selection.” Remote Sensing of Environment 221 (February): 260–73. https://doi.org/10.1016/j.rse.2018.11.026.\n\n\n“Measuring Performance: AUC (AUROC) – Glass Box.” n.d. Accessed March 12, 2024. https://glassboxmedicine.com/2019/02/23/measuring-performance-auc-auroc/.\n\n\nSchratz, Patrick, Jannes Muenchow, Eugenia Iturritxa, Jakob Richter, and Alexander Brenning. 2019. “Hyperparameter Tuning and Performance Assessment of Statistical and Machine-Learning Algorithms Using Spatial Data.” Ecological Modelling 406 (August): 109–20. https://doi.org/10.1016/j.ecolmodel.2019.06.002.\n\n\nShao, Zhenfeng, Tao Cheng, Huyan Fu, Deren Li, and Xiao Huang. 2023. “Emerging Issues in Mapping Urban Impervious Surfaces Using High-Resolution Remote Sensing Images.” Remote Sensing 15 (10, 10): 2562. https://doi.org/10.3390/rs15102562.\n\n\nTutorials, Earth Engine. n.d. “Interpreting Images - Accuracy Assessment Quantifying Classification Quality.” Accessed March 12, 2024. https://google-earth-engine.com/Interpreting-Images/Accuracy-Assessment-Quantifying-Classification-Quality/.\n\n\nZhang, Su, Fei Han, and Susan Bogus. 2020. Building Footprint and Height Information Extraction from Airborne LiDAR, Aerial Imagery, and Object-based Image Analysis. https://doi.org/10.1061/9780784482865.035.\n\n\nZhou, Weiqi, Austin Troy, and Morgan Grove. 2008. “Modeling Residential Lawn Fertilization Practices: Integrating High Resolution Remote Sensing with Socioeconomic Data.” Environmental Management 41 (5): 742–52. https://doi.org/10.1007/s00267-007-9032-z."
  },
  {
    "objectID": "week9.html#summary",
    "href": "week9.html#summary",
    "title": "8  Synthetic Aperture Radar (SAR)",
    "section": "8.1 Summary",
    "text": "8.1 Summary\nSynthetic Aperture Radar in active sensor that can ‘see’ through the atmosphere including clouds and gives our surface texture data. It emits an electromagnetic signal and records the amount of signal that bounces back.\nIt is able to capture a large area while moving by imaging pixels in swath multiple times (sweeping and moving), the collection of which will be combined to make ‘synthetic aperture’. Different wavelengths are used for different applications because it can affect scattering (3cm - 65 cm)\n\n\n\nDifferent wavelengths for different applications. Source: Earth Science Data Systems (2020)\n\n\nSAR signal contains two types of data: Amplitude and Phase\n\nFor Amplitude, besides the ‘strength’ and the wavelength of the backscatter it receives, polarisation is taken into accounts, that is, the orientation of the plane of transmission and that of reception. It could be single (Horizontal or Vertical only), Dual, Mixed (HV, VH). Different surfaces respond differently to the polarizations:\n\nRough scattering (e.g. bare earth) is most sensitive to VV\nVolume scattering (e.g. leaves) is most sensitive to mixed, VH or HV\nDouble bounce (e.g. trees / buildings) is most sensitive to HH.\n\n\n\n\n\nSource: Earth Science Data Systems (2020)\n\n\n\nFor phase, it records the location of wave on the cycle when the comes back to the sensor to determine elevation. Manipulating phase data using InSAR, essentially combines more SAR images over the same region, can reveal topography or motion (the filed is called interferometry). If phase shift comes from topography, and we would like to remove the effect of this, we are looking at DInSAR (the field is called differential inteferometry).\n\n\n\n\nSource: “Interferometry | Get to Know SAR – NASA-ISRO SAR Mission (NISAR)” (n.d.)\n\n\nLastly, SAR data products also come in different flavours. Raw data from Sentinel-1 is povided on a power scale, not suitable for visualisation (too bright, too dark). An amplitude scale (square root of the power scale) allows better visualisation. On the other hand, SAR data provided in GEE is on Decibel scale, making it good for identifying differences in dark pixels but not great for visualisation or statistical analysis."
  },
  {
    "objectID": "week9.html#application",
    "href": "week9.html#application",
    "title": "8  Synthetic Aperture Radar (SAR)",
    "section": "8.2 Application",
    "text": "8.2 Application\nSAR is a powerful type of sensor. Unlike optical sensors, SAR can operate day and night and can penetrate clouds, making it a reliable data source regardless of weather conditions. SAR backscatter is also sensitive to the shapes and materials of urban objects, allowing for detailed characterization of buildings and infrastructure. Finally, SAR data can be integrated with other types of remote sensing data, such as optical imagery, to provide a more comprehensive picture of the urban environment. Issues remain, however, pertaining to its relative complexity in interpretation and likelihood of noise but in many applications, the benefits outweigh the setbacks.\nSAR offers valuable capabilities for urban analytics such as:\n\nUrban Mapping and Change Detection: SAR data can be used to create detailed maps of urban areas, including buildings, roads, and other infrastructure. By comparing SAR images from different time periods, researchers can track urban growth and sprawl. In a study by Li et al. (2019), SAR for urban change detection used a mixed method combining subtraction and log-ratioing. Then, a Residual U-Net algorithm was trained to detect different extents of urban change, chosen over other image classification architectures for its ability to learn from complex, details-laden images of cities.\n\n\n\n\nUrban change detection methodology using SAR images and ResUNet by Li et al. (2019)\n\n\n\nLand Cover Classification: By analyzing the backscattered radar signal, SAR can classify different land cover types within an urban environment, such as vegetation, bare soil, impervious surfaces, and water. Although LULC classification can be done with optical imagery, as seen in previous entries, SAR imagery, especially with phase data, has the advantage of also detecting slight geological changes imperceptible to other sensors. However, there is no reason for them to be mutually exclusive. Shi et al. (2020) combined Sentinel-1 and Landsat-8 data to perform land cover classification in multiple Asian cities. The results showed superior accuracy compared to using just one of the sensors alone.\n\nMany research works acknowledged and attempted to overcome SAR’s shortcomings speckle noise). Huang et al. (2023) introduced a new dual-polar radar vegetation index (DpRVIm) that considers scattering information and terrain factors. The results show that the new method achieves higher accuracy than traditional methods, especially for farmland and forest. This shows that analysts should not feel bogged down by any seeming limitations but instead have many options to mix and match RS data and classification methodology to accomplish a task."
  },
  {
    "objectID": "week9.html#reflection",
    "href": "week9.html#reflection",
    "title": "8  Synthetic Aperture Radar (SAR)",
    "section": "8.3 Reflection",
    "text": "8.3 Reflection\nIt was fascinating to learn about SAR beyond what I had learned about LiDAR in Week 2. Both are active sensors that use radio frequency emissions to remotely sense the environment. Active sensors are fascinating because they are (usually) specific to certain applications rather than optical imagery that is quite versatile and often free-access. Therefore, this could be a very active field with new technologies emerging as industry needs arise.\nExamples of active sensors from besides SAR and LiDAR are:\n\nLaser and Radar Altimeter: measures altitude, which is the distance of an object above a fixed level, most commonly sea level.\nScatterometer: A specialized radar that uses high-frequency radio waves to estimate wind speed and direction over oceans.\nSounder: This device examines the atmosphere by analyzing reflected radio waves. It measures things like rain, temperature, humidity, and cloud types.\n\nWorth noting is that many such active remote sensors can be terrestrial and even generated by hand-held devices instead of mounted in space, opening up a world of possibilities for vast, hyper-local remote sensing data sets. Zhang et al. (2019)\n\n\n\n\nEarth Science Data Systems, NASA. 2020. “What Is Synthetic Aperture Radar? | Earthdata.” Backgrounder. Earth Science Data Systems, NASA. April 10, 2020. https://www.earthdata.nasa.gov/learn/backgrounders/what-is-sar.\n\n\nHuang, Yabo, Mengmeng Meng, Zhuoyan Hou, Lin Wu, Zhengwei Guo, Xiajiong Shen, Wenkui Zheng, and Ning Li. 2023. “Land Cover Classification of SAR Based on 1DCNN-MRF Model Using Improved Dual-Polarization Radar Vegetation Index.” Remote Sensing 15 (13, 13): 3221. https://doi.org/10.3390/rs15133221.\n\n\n“Interferometry | Get to Know SAR – NASA-ISRO SAR Mission (NISAR).” n.d. Accessed March 12, 2024. https://nisar.jpl.nasa.gov/mission/get-to-know-sar/interferometry/.\n\n\nLi, Lu, Chao Wang, Hong Zhang, Bo Zhang, and Fan Wu. 2019. “Urban Building Change Detection in SAR Images Using Combined Differential Image and Residual U-Net Network.” Remote Sensing 11 (9, 9): 1091. https://doi.org/10.3390/rs11091091.\n\n\nShi, Xiaofei, Zhiyu Deng, Xing Ding, and Li Li. 2020. “Land Cover Classification Combining Sentinel-1 and Landsat 8 Imagery Driven by Markov Random Field with Amendment Reliability Factors.” EURASIP Journal on Wireless Communications and Networking 2020 (1): 87. https://doi.org/10.1186/s13638-020-01713-5.\n\n\nZhang, Jiayi, Xia Liu, Yan Liang, Qiang Cao, Yongchao Tian, Yan Zhu, Weixing Cao, and Xiaojun Liu. 2019. “Using a Portable Active Sensor to Monitor Growth Parameters and Predict Grain Yield of Winter Wheat.” Sensors 19 (5, 5): 1108. https://doi.org/10.3390/s19051108."
  },
  {
    "objectID": "glossary.html#basic",
    "href": "glossary.html#basic",
    "title": "Glossary",
    "section": "Basic::",
    "text": "Basic::\n\nActive Sensor: A remote sensing system that transmits its own energy (e.g., radio waves, light) towards the target and records the reflected or scattered radiation.\nAlbedo: The proportion of electromagnetic radiation reflected by a surface.\nAmplitude Data: In SAR imagery, refers to the strength of the reflected signal, used to distinguish between different land cover types based on their varying reflectivity properties.\nAnisotropy: Variation in reflectance depending on viewing angle.\nAzimuth: Horizontal direction of an object relative to an observer (degrees from north).\nBiosignature: Spectral signature indicative of biological activity.\nDEM (Digital Elevation Model): Digital representation of terrain elevation.\nGeometric Correction: Correcting distortions in image geometry.\nIncidence Angle: The angle at which radiation strikes a surface.\nLiDAR (Light Detection and Ranging): Remote sensing technique using lasers to measure distance.\nNadir: The point directly below an observer, on the opposite side of the Earth.\nPassive Sensor: A remote sensing system that detects the naturally occurring energy emitted (from the sun) or reflected from the Earth’s surface and atmosphere.\nPhase Data: In SAR imagery, refers to the information about the relative timing of the received signal compared to the transmitted signal, used to generate information about the surface features.\nRadiance: Total electromagnetic energy emitted per unit area, solid angle, and time.\nReflectance: The proportion of electromagnetic radiation reflected by a surface.\nResolution:\n\nSpatial: Size of an area represented by a single pixel.\nSpectral: Number and width of wavelength bands used in an image.\nTemporal: Frequency with which an area is revisited by a satellite sensor.\n\nSensor: A device that detects and measures electromagnetic radiation.\nSpectral Band: A specific range of wavelengths within the electromagnetic spectrum.\nSpectral Reflectance: The proportion of electromagnetic radiation reflected by a surface at different wavelengths.\nSpectral Signature: The unique way an object reflects or emits radiation across different wavelengths.\nSynthetic Aperture Radar (SAR): A type of radar that creates high-resolution images by processing reflected signals.\nZenith Angle: The angle between the vertical direction and a point in the sky."
  },
  {
    "objectID": "glossary.html#pre-processing",
    "href": "glossary.html#pre-processing",
    "title": "Glossary",
    "section": "Pre-processing:",
    "text": "Pre-processing:\n\nCalibration: Correcting for systematic errors in satellite imagery.\nRadiometric Correction: Correcting for variations in sensor response and atmospheric effects.\nGeometric Correction: Correcting distortions in image geometry caused by sensor perspective and Earth’s curvature.\nAtmospheric Correction: Correcting for the effects of atmospheric gases and aerosols on the spectral signature of objects in an image.\nContrast Stretching: Enhancing the visual contrast between features in an image.\nFiltering: Techniques for manipulating and enhancing image data based on specific criteria (e.g., spatial filtering to sharpen edges, spectral filtering to isolate specific wavelengths).\nBand Indices: Mathematical transformations of spectral bands used to highlight specific features (e.g., NDVI for vegetation health).\nData Fusion: Combining data from multiple sources (e.g., satellite imagery, LiDAR) for more comprehensive analysis."
  },
  {
    "objectID": "glossary.html#analysis",
    "href": "glossary.html#analysis",
    "title": "Glossary",
    "section": "Analysis:",
    "text": "Analysis:\n\nClassification: Assigning categories (e.g., forest, water) to pixels in an image based on their spectral characteristics.\nClustering: Grouping pixels with similar spectral characteristics together without predefined classes.\nChange Detection: Identifying and analyzing modifications in land cover or features over time using multi-temporal imagery.\nImage Classification: The process of assigning pixels or objects to categories (e.g., LULC)\nObject-Based Image Analysis (OBIA): Analyzing imagery based on segmented objects rather than individual pixels, allowing for incorporation of shape and spatial relationships.\nSLIC (Superpixels Lattice Segmentation): A segmentation algorithm used to divide an image into smaller, more uniform regions called superpixels.\nSpatial Autocorrelation: The degree to which the values at one location in a spatial dataset are statistically related to the values at neighboring locations.\nSpectral Mixture Analysis (SMA): Estimating the fractional abundance of different materials within a pixel based on their spectral signatures."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Barbato, Clare T., and Aaron L. Strong. 2023. “Farmer Perspectives\non Carbon Markets Incentivizing Agricultural Soil Carbon\nSequestration.” Npj Climate Action 2 (1, 1): 1–9. https://doi.org/10.1038/s44168-023-00055-4.\n\n\nBlaschke, Thomas, Geoffrey J. Hay, Maggi Kelly, Stefan Lang, Peter\nHofmann, Elisabeth Addink, Raul Queiroz Feitosa, et al. 2014.\n“Geographic Object-Based Image Analysis –\nTowards a New Paradigm.” ISPRS Journal of\nPhotogrammetry and Remote Sensing 87 (January): 180–91. https://doi.org/10.1016/j.isprsjprs.2013.09.014.\n\n\n“Carbon Farming: Opportunities for\nAgriculture and Farmers to Gain From\nDecarbonization.” n.d. Accessed February 15, 2024. https://www.spglobal.com/esg/insights/topics/carbon-farming-opportunities-for-agriculture-and-farmers-to-gain-from-decarbonization.\n\n\nColuzzi, Rosa, Vito Imbrenda, Maria Lanfredi, and Tiziana Simoniello.\n2018. “A First Assessment of the Sentinel-2 Level\n1-C Cloud Mask Product to Support Informed Surface\nAnalyses.” Remote Sensing of Environment 217 (November):\n426–43. https://doi.org/10.1016/j.rse.2018.08.009.\n\n\nDegerickx, J., D. A. Roberts, and B. Somers. 2019. “Enhancing the\nPerformance of Multiple Endmember Spectral Mixture Analysis\n(MESMA) for Urban Land Cover Mapping Using Airborne Lidar\nData and Band Selection.” Remote Sensing of Environment\n221 (February): 260–73. https://doi.org/10.1016/j.rse.2018.11.026.\n\n\n“Depth Matters for Soil Carbon Accounting –\nCarbonPlan.” n.d. Accessed February 15, 2024. https://carbonplan.org/research/soil-depth-sampling.\n\n\n“DestinE for Human Heat Stress: ECMWF\nUse Case to Tackle Urban Heat Islands.” n.d. Accessed January 29,\n2024. https://stories.ecmwf.int/destine-for-human-heat-stress-ecmwf-use-case-to-tackle-urban-heat-islands/.\n\n\nDvorakova, Klara, Uta Heiden, and Bas van Wesemael. 2021.\n“Sentinel-2 Exposed Soil Composite for Soil\nOrganic Carbon Prediction.” Remote Sensing 13 (9,\n9): 1791. https://doi.org/10.3390/rs13091791.\n\n\nEarth Science Data Systems, NASA. 2019. “What Is Remote\nSensing? | Earthdata.” Backgrounder. August\n23, 2019. https://www.earthdata.nasa.gov/learn/backgrounders/remote-sensing.\n\n\n———. 2020. “What Is Synthetic Aperture Radar? |\nEarthdata.” Backgrounder. Earth Science Data\nSystems, NASA. April 10, 2020. https://www.earthdata.nasa.gov/learn/backgrounders/what-is-sar.\n\n\nEjaz, Nuaman, Jarbou Bahrawi, Khalid Mohammed Alghamdi, Khalil Ur\nRahman, and Songhao Shang. 2023. “Drought Monitoring Using\nLandsat Derived Indices and Google Earth Engine\nPlatform: A Case Study from Al-Lith\nWatershed, Kingdom of Saudi\nArabia.” Remote Sensing 15 (4, 4): 984. https://doi.org/10.3390/rs15040984.\n\n\n“Google Earth Engine Comparison Between Python and\nJavaScript | Bikesh Bade.” n.d.\nAccessed March 7, 2024. https://bikeshbade.com.np/tutorials/Detail/?title=Google%20earth%20engine%20Detail%20comparison%20between%20Python%20and%20JavaScript&code=9.\n\n\nGuo, Yiqing, Xiuping Jia, and David Paull. 2017. “A\nDomain-Transfer Support Vector Machine for Multi-Temporal Remote Sensing\nImagery Classification.” 2017 IEEE International Geoscience\nand Remote Sensing Symposium (IGARSS), July, 2215–18. https://doi.org/10.1109/IGARSS.2017.8127428.\n\n\nHeiden, Uta, Pablo d’Angelo, Peter Schwind, Paul Karlshöfer, Rupert\nMüller, Simone Zepp, Martin Wiesmeier, and Peter Reinartz. 2022.\n“Soil Reflectance Composites—Improved\nThresholding and Performance Evaluation.”\nRemote Sensing 14 (18, 18): 4526. https://doi.org/10.3390/rs14184526.\n\n\nHuang, Yabo, Mengmeng Meng, Zhuoyan Hou, Lin Wu, Zhengwei Guo, Xiajiong\nShen, Wenkui Zheng, and Ning Li. 2023. “Land Cover\nClassification of SAR Based on 1DCNN-MRF Model\nUsing Improved Dual-Polarization Radar Vegetation Index.”\nRemote Sensing 15 (13, 13): 3221. https://doi.org/10.3390/rs15133221.\n\n\n“Hutton Soil Scientists Help Develop First Global Soil\nOrganic Carbon Map | The James Hutton\nInstitute.” n.d. Accessed February 15, 2024. https://www.hutton.ac.uk/news/hutton-soil-scientists-help-develop-first-global-soil-organic-carbon-map.\n\n\n“Interferometry | Get to Know SAR –\nNASA-ISRO SAR Mission (NISAR).” n.d.\nAccessed March 12, 2024. https://nisar.jpl.nasa.gov/mission/get-to-know-sar/interferometry/.\n\n\nJansen, Louisa J. M, and Antonio Di Gregorio. 2003. “Land-Use Data\nCollection Using the ‘Land Cover Classification System’:\nResults from a Case Study in Kenya.” Land Use\nPolicy 20 (2): 131–48. https://doi.org/10.1016/S0264-8377(02)00081-9.\n\n\nKganyago, Mahlatse, and Phila Sibandze. 2014. Evaluating the\nPerformance of Pixel-based Vs. Object-based Classifiers for Extracting High\nResolution Land Cover Product from SPOT 6 Imagery\n(Non-peer Reviewed). https://doi.org/10.13140/2.1.4177.6002.\n\n\nLi, Lu, Chao Wang, Hong Zhang, Bo Zhang, and Fan Wu. 2019. “Urban\nBuilding Change Detection in SAR Images Using\nCombined Differential Image and Residual U-Net\nNetwork.” Remote Sensing 11 (9, 9): 1091. https://doi.org/10.3390/rs11091091.\n\n\nLi, Wangbin, Kaimin Sun, Hepeng Zhao, Wenzhuo Li, Jinjiang Wei, and Song\nGao. 2022. “Extracting Buildings from High-Resolution Remote\nSensing Images by Deep ConvNets Equipped with\nStructural-Cue-Guided Feature Alignment.” International\nJournal of Applied Earth Observation and Geoinformation 113\n(September): 102970. https://doi.org/10.1016/j.jag.2022.102970.\n\n\nMacedo, Fabrício L., Adélia M. O. Sousa, Ana Cristina Gonçalves, José R.\nMarques da Silva, Paulo A. Mesquita, and Ricardo A. F. Rodrigues. 2018.\n“Above-Ground Biomass Estimation for Quercus\nRotundifolia Using Vegetation Indices Derived from High Spatial\nResolution Satellite Images.” European Journal of Remote\nSensing 51 (1): 932–44. https://doi.org/10.1080/22797254.2018.1521250.\n\n\nMai Sy, Hung, Chinh Luu, Quynh Duy Bui, Hang Ha, and Dinh Quoc Nguyen.\n2023. “Urban Flood Risk Assessment Using Sentinel-1\non the Google Earth Engine: A Case Study in Thai\nNguyen City, Vietnam.” Remote Sensing\nApplications: Society and Environment 31 (August): 100987. https://doi.org/10.1016/j.rsase.2023.100987.\n\n\n“Measuring Performance: AUC\n(AUROC) – Glass Box.” n.d. Accessed\nMarch 12, 2024. https://glassboxmedicine.com/2019/02/23/measuring-performance-auc-auroc/.\n\n\nMohammed, Ariful Khan, Angana Kuri, Sohag Ahammed, Kazi Al Muqtadir\nAbir, and Mohammed A. S. Arfin-Khan. 2023. “A Google Earth Engine\nApproach for Anthropogenic Forest Fire Assessment with Remote Sensing\nData in Rema-Kalenga Wildlife Sanctuary,\nBangladesh.” Geology, Ecology, and\nLandscapes 0 (0): 1–22. https://doi.org/10.1080/24749508.2023.2165297.\n\n\nMöller, Markus, Simone Zepp, Martin Wiesmeier, Heike Gerighausen, and\nUta Heiden. 2022. “Scale-Specific Prediction of\nTopsoil Organic Carbon Contents Using Terrain Attributes\nand SCMaP Soil Reflectance Composites.” Remote\nSensing 14 (10, 10): 2295. https://doi.org/10.3390/rs14102295.\n\n\nPérez-Cutillas, Pedro, Alberto Pérez-Navarro, Carmelo Conesa-García,\nDemetrio Antonio Zema, and Jesús Pilar Amado-Álvarez. 2023. “What\nIs Going on Within Google Earth Engine? A Systematic Review\nand Meta-Analysis.” Remote Sensing Applications: Society and\nEnvironment 29 (January): 100907. https://doi.org/10.1016/j.rsase.2022.100907.\n\n\nRavanelli, Roberta, Andrea Nascetti, Raffaella Valeria Cirigliano,\nClarissa Di Rico, Giovanni Leuzzi, Paolo Monti, and Mattia Crespi. 2018.\n“Monitoring the Impact of Land Cover\nChange on Surface Urban Heat Island Through\nGoogle Earth Engine: Proposal of a\nGlobal Methodology, First Applications and\nProblems.” Remote Sensing 10 (9, 9): 1488.\nhttps://doi.org/10.3390/rs10091488.\n\n\n“Remote Sensing, Satellite Imaging\nTechnology | Satellite Imaging Corp.” n.d.\nAccessed January 27, 2024. https://www.satimagingcorp.com/services/resources/characterization-of-satellite-remote-sensing-systems/.\n\n\nSchratz, Patrick, Jannes Muenchow, Eugenia Iturritxa, Jakob Richter, and\nAlexander Brenning. 2019. “Hyperparameter Tuning and Performance\nAssessment of Statistical and Machine-Learning Algorithms Using Spatial\nData.” Ecological Modelling 406 (August): 109–20. https://doi.org/10.1016/j.ecolmodel.2019.06.002.\n\n\nSchumann, Guy, Laura Giustarini, Angelica Tarpanelli, Ben Jarihani, and\nSandro Martinis. 2023. “Flood Modeling and\nPrediction Using Earth Observation Data.”\nSurveys in Geophysics 44 (5): 1553–78. https://doi.org/10.1007/s10712-022-09751-y.\n\n\nShao, Zhenfeng, Tao Cheng, Huyan Fu, Deren Li, and Xiao Huang. 2023.\n“Emerging Issues in Mapping Urban Impervious\nSurfaces Using High-Resolution Remote Sensing Images.”\nRemote Sensing 15 (10, 10): 2562. https://doi.org/10.3390/rs15102562.\n\n\nShi, Xiaofei, Zhiyu Deng, Xing Ding, and Li Li. 2020. “Land Cover\nClassification Combining Sentinel-1 and\nLandsat 8 Imagery Driven by Markov Random\nField with Amendment Reliability Factors.” EURASIP Journal on\nWireless Communications and Networking 2020 (1): 87. https://doi.org/10.1186/s13638-020-01713-5.\n\n\nTalukdar, Swapan, Pankaj Singha, Susanta Mahato, Shahfahad, Swades Pal,\nYuei-An Liou, and Atiqur Rahman. 2020. “Land-Use Land-Cover\nClassification by Machine Learning Classifiers for\nSatellite Observations—A Review.”\nRemote Sensing 12 (7, 7): 1135. https://doi.org/10.3390/rs12071135.\n\n\nTutorials, Earth Engine. n.d. “Interpreting Images -\nAccuracy Assessment Quantifying Classification\nQuality.” Accessed March 12, 2024. https://google-earth-engine.com/Interpreting-Images/Accuracy-Assessment-Quantifying-Classification-Quality/.\n\n\nTziolas, Nikolaos, Nikolaos Tsakiridis, Sabine Chabrillat, José A. M.\nDemattê, Eyal Ben-Dor, Asa Gholizadeh, George Zalidis, and Bas van\nWesemael. 2021. “Earth Observation Data-Driven Cropland Soil\nMonitoring: A Review.” Remote\nSensing 13 (21, 21): 4439. https://doi.org/10.3390/rs13214439.\n\n\nUrbina-Salazar, Diego, Emmanuelle Vaudour, Nicolas Baghdadi, Eric\nCeschia, Anne C. Richer-de-Forges, Sébastien Lehmann, and Dominique\nArrouays. 2021. “Using Sentinel-2 Images for\nSoil Organic Carbon Content Mapping in\nCroplands of Southwestern France. The\nUsefulness of Sentinel-1/2 Derived Moisture\nMaps and Mismatches Between Sentinel\nImages and Sampling Dates.” Remote\nSensing 13 (24, 24): 5115. https://doi.org/10.3390/rs13245115.\n\n\nWang, Haibo, Xueshuang Gong, Bingbing Wang, Chao Deng, and Qiong Cao.\n2021. “Urban Development Analysis Using Built-up Area Maps Based\non Multiple High-Resolution Satellite Data.” International\nJournal of Applied Earth Observation and Geoinformation 103\n(December): 102500. https://doi.org/10.1016/j.jag.2021.102500.\n\n\nWang, Yixuan, Xuecao Li, Peiyi Yin, Guojiang Yu, Wenting Cao, Jinxiu\nLiu, Lin Pei, et al. 2023. “Characterizing Annual Dynamics of\nUrban Form at the Horizontal and Vertical Dimensions Using Long-Term\nLandsat Time Series Data.” ISPRS Journal of\nPhotogrammetry and Remote Sensing 203 (September): 199–210. https://doi.org/10.1016/j.isprsjprs.2023.07.025.\n\n\nWesemael, Bas van, Sabine Chabrillat, Adrian Sanz Dias, Michael Berger,\nand Zoltan Szantoi. 2023. “Remote Sensing for\nSoil Organic Carbon Mapping and\nMonitoring.” Remote Sensing 15 (14, 14):\n3464. https://doi.org/10.3390/rs15143464.\n\n\nZepp, Simone, Uta Heiden, Martin Bachmann, Martin Wiesmeier, Michael\nSteininger, and Bas van Wesemael. 2021. “Estimation of Soil\nOrganic Carbon Contents in Croplands of\nBavaria from SCMaP Soil Reflectance\nComposites.” Remote Sensing 13 (16, 16): 3141. https://doi.org/10.3390/rs13163141.\n\n\nZhang, Jiayi, Xia Liu, Yan Liang, Qiang Cao, Yongchao Tian, Yan Zhu,\nWeixing Cao, and Xiaojun Liu. 2019. “Using a Portable Active\nSensor to Monitor Growth Parameters and\nPredict Grain Yield of Winter Wheat.”\nSensors 19 (5, 5): 1108. https://doi.org/10.3390/s19051108.\n\n\nZhang, Su, Fei Han, and Susan Bogus. 2020. Building\nFootprint and Height Information Extraction\nfrom Airborne LiDAR, Aerial Imagery, and Object-based Image Analysis. https://doi.org/10.1061/9780784482865.035.\n\n\nZhou, Weiqi, Austin Troy, and Morgan Grove. 2008. “Modeling\nResidential Lawn Fertilization Practices: Integrating\nHigh Resolution Remote Sensing with Socioeconomic\nData.” Environmental Management 41 (5): 742–52.\nhttps://doi.org/10.1007/s00267-007-9032-z."
  },
  {
    "objectID": "glossary.html#basics",
    "href": "glossary.html#basics",
    "title": "Glossary",
    "section": "Basics:",
    "text": "Basics:\n\nActive Sensor: A remote sensing system that transmits its own energy (e.g., radio waves, light) towards the target and records the reflected or scattered radiation.\nAlbedo: The proportion of electromagnetic radiation reflected by a surface.\nAmplitude Data: In SAR imagery, refers to the strength of the reflected signal, used to distinguish between different land cover types based on their varying reflectivity properties.\nAnisotropy: Variation in reflectance depending on viewing angle.\nAzimuth: Horizontal direction of an object relative to an observer (degrees from north).\nBiosignature: Spectral signature indicative of biological activity.\nDEM (Digital Elevation Model): Digital representation of terrain elevation.\nGeometric Correction: Correcting distortions in image geometry.\nIncidence Angle: The angle at which radiation strikes a surface.\nLiDAR (Light Detection and Ranging): Remote sensing technique using lasers to measure distance.\nNadir: The point directly below an observer, on the opposite side of the Earth.\nPassive Sensor: A remote sensing system that detects the naturally occurring energy emitted (from the sun) or reflected from the Earth’s surface and atmosphere.\nPhase Data: In SAR imagery, refers to the information about the relative timing of the received signal compared to the transmitted signal, used to generate information about the surface features.\nRadiance: Total electromagnetic energy emitted per unit area, solid angle, and time.\nReflectance: The proportion of electromagnetic radiation reflected by a surface.\nResolution:\n\nSpatial: Size of an area represented by a single pixel.\nSpectral: Number and width of wavelength bands used in an image.\nTemporal: Frequency with which an area is revisited by a satellite sensor.\n\nSensor: A device that detects and measures electromagnetic radiation.\nSpectral Band: A specific range of wavelengths within the electromagnetic spectrum.\nSpectral Reflectance: The proportion of electromagnetic radiation reflected by a surface at different wavelengths.\nSpectral Signature: The unique way an object reflects or emits radiation across different wavelengths.\nSynthetic Aperture Radar (SAR): A type of radar that creates high-resolution images by processing reflected signals.\nZenith Angle: The angle between the vertical direction and a point in the sky."
  }
]