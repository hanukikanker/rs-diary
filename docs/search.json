[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Viewed from Above: An Learning Diary on Earth Observation",
    "section": "",
    "text": "A Novice’s guide to Remote Sensing and Earth Observation\nFrom another novice!\nIntro [TBD]"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "This is a book created from markdown and executable code.\nSee Knuth (1984) for additional discussion of literate programming.\n\n1 + 1\n\n[1] 2\n\n\n\n\n\n\nKnuth, Donald E. 1984. “Literate Programming.” Comput. J. 27 (2): 97–111. https://doi.org/10.1093/comjnl/27.2.97."
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever.\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Earth Science Data Systems, NASA. 2019. “What Is Remote\nSensing? | Earthdata.” Backgrounder. August\n23, 2019. https://www.earthdata.nasa.gov/learn/backgrounders/remote-sensing.\n\n\nErrea, Cristina Laurenti, Cátia Rodrigues de Almeida, Artur Gonçalves,\nand Ana Cláudia Teodoro. 2023. “Remote Sensing\nAnalysis of the Surface Urban Heat Island Effect in\nVitoria-Gasteiz, 1985 to 2021.” Remote\nSensing 15 (12, 12): 3110. https://doi.org/10.3390/rs15123110.\n\n\n“Remote Sensing, Satellite Imaging\nTechnology | Satellite Imaging Corp.” n.d.\nAccessed January 27, 2024. https://www.satimagingcorp.com/services/resources/characterization-of-satellite-remote-sensing-systems/.\n\n\nSchumann, Guy, Laura Giustarini, Angelica Tarpanelli, Ben Jarihani, and\nSandro Martinis. 2023. “Flood Modeling and\nPrediction Using Earth Observation Data.”\nSurveys in Geophysics 44 (5): 1553–78. https://doi.org/10.1007/s10712-022-09751-y.\n\n\nWang, Haibo, Xueshuang Gong, Bingbing Wang, Chao Deng, and Qiong Cao.\n2021. “Urban Development Analysis Using Built-up Area Maps Based\non Multiple High-Resolution Satellite Data.” International\nJournal of Applied Earth Observation and Geoinformation 103\n(December): 102500. https://doi.org/10.1016/j.jag.2021.102500."
  },
  {
    "objectID": "week1.html#summary",
    "href": "week1.html#summary",
    "title": "2  Entering: The World of Remote Sensing",
    "section": "2.1 Summary:",
    "text": "2.1 Summary:\nImagine sophisticated sensors mounted on these celestial companions, each like a specialized listener collecting whispers from the Earth below. Some passively listen to sunlight reflected off our planet’s surface, while others actively send their own signals and capture the echoes. These sensors interpret a fascinating language of light, both invisible and visible, known as the electromagnetic spectrum. Different materials sing unique tunes in this spectrum, allowing us to identify them - it’s like having a scientific decoder ring for Earth’s secrets!\nBut information isn’t just about color. Remote sensing data has its own “resolution” recipe, encompassing:\n\nSpectral: How many “voices” the sensor can hear, revealing more detail with each additional band.\nSpatial: The size of each pixel in the image, ranging from centimeters to kilometers, offering varying levels of detail.\nTemporal: How often the sensor revisits the same area, providing a dynamic view of changes over time.\nRadiometric: The range of brightness levels captured, painting a vibrant and accurate picture.\n\nThese resolutions work together, creating a nuanced and informative snapshot of our planet."
  },
  {
    "objectID": "week1.html#applications",
    "href": "week1.html#applications",
    "title": "1  Entering the World of Remote Sensing",
    "section": "1.2 Applications",
    "text": "1.2 Applications\nRemote Sensing has many transformative applications in the realm of Urban Analytics. Think of it as an X-ray for cities, revealing hidden patterns and empowering informed decision-making by city planners, urban designers, and public officials to make swift and informed decisions to improve the lives of millions of urbanites.\nHere are some examples\n\nMapping Urban Growth: By tracking changes in the built environment over time, we can identify sprawling suburbs, monitor urban expansion, and plan for infrastructure needs. Remote sensing data are particularly rich sources to train classification models to extract features and identify unmapped communities.\n\nPredicting Floods with Foresight: Analysing land cover and terrain, it anticipates where water will flow, safeguarding communities from harm, while also estimating potential damage. Here is an example of how insurance companies can make use of such data to assess risk and process claims.\n\nEnergy Efficiency: Identifying heat island effects and understanding building energy consumption through thermal imaging empowers planners to design sustainable cities, and researchers to determine the best strategies to combat long term climate disasters.\n\n\nThis is just the beginning. Remote sensing is transforming the way we understand and manage our cities, paving the way for a healthier, smarter, and more sustainable urban future."
  },
  {
    "objectID": "week1.html#reflection",
    "href": "week1.html#reflection",
    "title": "2  Entering: The World of Remote Sensing",
    "section": "2.3 Reflection:",
    "text": "2.3 Reflection:\nMy first foray into this world was both humbling and exhilarating. Processing vast amounts of data, juggling resolutions, and diving into the electromagnetic spectrum symphony - it was a challenging yet rewarding journey. Every step felt like decoding a new secret, like learning a new language of our planet.\nWorking with satellite imagery was unlike anything I’d experienced before. It shifted my perspective from high-resolution photographs to spectral dances, where each pixel held a universe of information. Building a true-color composite from RGB bands, analyzing plots in Cape Town, and unraveling the spectral signatures - it was like seeing the world through a brand-new lens.\nThis journey ignited a spark of curiosity that burns brighter with each step. I’m eager to delve deeper into advanced platforms like Google Earth Engine, explore the intricate web of policy applications, and unlock the true potential of this technology for the betterment of our planet.\nJoin me on this thrilling expedition! Together, let’s paint a new picture of our planet, pixel by pixel, and use remote sensing to write a better story for our shared future.\nThis revised version retains the informative nature of the original post while removing sensational language and cheese. It also incorporates relevant images to enhance understanding and engagement. Remember to further personalize it by adding your own experiences and specific areas of interest within remote sensing!"
  },
  {
    "objectID": "week1.html#demystifying-jargons",
    "href": "week1.html#demystifying-jargons",
    "title": "2  Entering the World of Remote Sensing",
    "section": "2.1 Demystifying jargons",
    "text": "2.1 Demystifying jargons\nSensors, well, ‘sense’ Earth in two ways: Some passively listen to sunlight reflected off our planet’s surface (i.e., passive sensors), while others actively send their own signals and capture the echoes (i.e., active sensors).\n\n\n\nActive vs Passive Remote Sensing (Earth Science Data Systems 2019).\n\n\nThese sensors interpret a fascinating language of light, both invisible and visible, known as the electromagnetic spectrum. Different materials emit unique tunes in this spectrum, allowing us to identify them, like decoding DNA!\n\n\n\nThe Electromagnetic Spectrum (EMS). Credit: NASA Science\n\n\nBut information isn’t just about color. Remote sensing data has its own “resolution” recipe, encompassing:\n\nSpectral: How many “voices” (i.e., the range within the EMS) the sensor can hear, revealing more detail with each additional band.\nSpatial: The size of each pixel in the image, ranging from centimeters to kilometers, offering varying levels of detail.\nTemporal: How often the sensor revisits the same area, providing a dynamic view of changes over time.\nRadiometric: The range of brightness levels captured, painting a vibrant and accurate picture.\n\n[pic]\nThese resolutions work together, creating a nuanced and informative snapshot of our planet."
  },
  {
    "objectID": "week1.html#reflections",
    "href": "week1.html#reflections",
    "title": "1  Remote Sensing: To see the unseen",
    "section": "1.3 Reflections",
    "text": "1.3 Reflections\nMy first foray into the world of Remote Sensing was eye-opening, to say the least. For the uninitiated, it is easy to assume that remote sensing purely means orthophotographic satellite images that one might see using platforms such as Google Maps, i.e., as if the only thing that sensors do were to snap a simple photo of the planet like a phone camera.\nIn reality, it unlocks unseen depths of data beyond mere human perception, less high-resolution photographs and more spectral signatures, where each pixel holds a universe of information. Building a true-color composite from all these layers (so that our eyes can see) was like seeing the world through a brand-new lens.\nI am particularly excited to get started with Google Earth Engine later on as the primary gateway to access the wealth of remote sensing data and analytics with more ease in order to solve specific problems facing our world today.\n\n\n\n\n“DestinE for Human Heat Stress: ECMWF Use Case to Tackle Urban Heat Islands.” n.d. Accessed January 29, 2024. https://stories.ecmwf.int/destine-for-human-heat-stress-ecmwf-use-case-to-tackle-urban-heat-islands/.\n\n\nEarth Science Data Systems, NASA. 2019. “What Is Remote Sensing? | Earthdata.” Backgrounder. August 23, 2019. https://www.earthdata.nasa.gov/learn/backgrounders/remote-sensing.\n\n\n“Remote Sensing, Satellite Imaging Technology | Satellite Imaging Corp.” n.d. Accessed January 27, 2024. https://www.satimagingcorp.com/services/resources/characterization-of-satellite-remote-sensing-systems/.\n\n\nSchumann, Guy, Laura Giustarini, Angelica Tarpanelli, Ben Jarihani, and Sandro Martinis. 2023. “Flood Modeling and Prediction Using Earth Observation Data.” Surveys in Geophysics 44 (5): 1553–78. https://doi.org/10.1007/s10712-022-09751-y.\n\n\nWang, Haibo, Xueshuang Gong, Bingbing Wang, Chao Deng, and Qiong Cao. 2021. “Urban Development Analysis Using Built-up Area Maps Based on Multiple High-Resolution Satellite Data.” International Journal of Applied Earth Observation and Geoinformation 103 (December): 102500. https://doi.org/10.1016/j.jag.2021.102500."
  },
  {
    "objectID": "week1.html",
    "href": "week1.html",
    "title": "2  Week 1",
    "section": "",
    "text": "3 Entering the World of Remote Sensing\nRemote Sensing is the use of satellites, planes, drones, etc. as our aerial eyes, piecing together a portrait of our planet through light and data and revolutionising the way we understand and interact with it."
  },
  {
    "objectID": "week1.html#in-a-few-words",
    "href": "week1.html#in-a-few-words",
    "title": "1  Entering the World of Remote Sensing",
    "section": "1.1 In a few words…",
    "text": "1.1 In a few words…\nSensors, well, ‘sense’ Earth in two ways: Some passively listen to sunlight reflected off our planet’s surface (i.e., passive sensors), while others actively send their own signals and capture the echoes (i.e., active sensors). By that definition, the human eye is a type of passive sensors!\n\n\n\nActive vs Passive Remote Sensing (Earth Science Data Systems 2019).\n\n\nThese sensors interpret a fascinating language of light, both invisible and visible, known as the electromagnetic spectrum.\n\n\n\nThe Electromagnetic Spectrum (EMS). Credit: NASA Science\n\n\nElectromagnetic radiation moves as waves as perpendicular electric and magnetic field with a wavelength: λ = c/v where:\n\nλ = wavelength, distance between two crests\nc = velocity of light 3 x 108 m/sec\nv = frequency, rate of oscillation (full oscillations in a time unit)\n\nDifferent materials reflect unique wavelengths in this spectrum, allowing us to identify them, like decoding DNA! More on this later…\n\n\n\nWavelength vs. Oscillation\n\n\nBut information that sensors receive isn’t just about color. Remote sensing data has its own “resolution” recipe, encompassing:\n\nSpectral: How EMS bands (i.e., the range within the EMS) the sensor can hear, revealing more detail with each additional band.\nSpatial: The size of each pixel in the image, ranging from centimeters to kilometers, offering varying levels of detail.\nTemporal: How often the sensor revisits the same area, providing a dynamic view of changes over time.\nRadiometric: The range of brightness levels captured, painting a vibrant and accurate picture.\n\nIn reality, depending on the purpose, each sensor is equip to have better resolution of one type over the other. For example, sensors with a high spatial resolution of 5m (i.e., each pixel is 10x10 on the ground) will have lower spectral resolution (i.e., capturing only narrow range of the EMS) (“Remote Sensing, Satellite Imaging Technology | Satellite Imaging Corp” n.d.)"
  },
  {
    "objectID": "week2.html",
    "href": "week2.html",
    "title": "2  Presenting…LiDAR",
    "section": "",
    "text": "Having understood how sensors work in theory, we can now shift our view to appreciate how one of them in particular, LiDAR (Light Detection and Ranging), is deployed in practice and can benefit us in an emerging sector: Autonomous Vehicles."
  },
  {
    "objectID": "week1.html#in-just-a-few-words",
    "href": "week1.html#in-just-a-few-words",
    "title": "1  Remote Sensing: To see the unseen",
    "section": "1.1 In just a few words…",
    "text": "1.1 In just a few words…\nRemote Sensing is the use of satellites, planes, drones, etc. as our aerial eyes, piecing together a portrait of our planet through light and data and revolutionising the way we understand and interact with it.\nSensors, well, ‘sense’ Earth in two ways: Some passively listen to sunlight reflected off our planet’s surface (i.e., passive sensors), while others actively send their own signals and capture the echoes (i.e., active sensors). By that definition, the human eye is a type of passive sensors!\n\n\n\nActive vs Passive Remote Sensing (Earth Science Data Systems 2019).\n\n\nThese sensors interpret a fascinating language of light, both invisible and visible, known as the electromagnetic spectrum.\n\n\n\nThe Electromagnetic Spectrum (EMS). Credit: NASA Science\n\n\nElectromagnetic radiation moves as waves as perpendicular electric and magnetic field with a wavelength: λ = c/v where:\n\nλ = wavelength, distance between two crests\nc = velocity of light 3 x 108 m/sec\nv = frequency, rate of oscillation (full oscillations in a time unit)\n\nDifferent materials reflect unique wavelengths in this spectrum, allowing us to identify them, like decoding DNA! More on this later…\n\n\n\nWavelength vs. Oscillation\n\n\nBut information that sensors receive isn’t just about color. Remote sensing data has its own “resolution” recipe, encompassing:\n\nSpectral: How EMS bands (i.e., the range within the EMS) the sensor can hear, revealing more detail with each additional band.\nSpatial: The size of each pixel in the image, ranging from centimeters to kilometers, offering varying levels of detail.\nTemporal: How often the sensor revisits the same area, providing a dynamic view of changes over time.\nRadiometric: The range of brightness levels captured, painting a vibrant and accurate picture.\n\nIn reality, depending on the purpose, each sensor is equip to have better resolution of one type over the other. For example, sensors with a high spatial resolution of 5m (i.e., each pixel is 10x10 on the ground) will have lower spectral resolution (i.e., capturing only narrow range of the EMS) (“Remote Sensing, Satellite Imaging Technology | Satellite Imaging Corp” n.d.)"
  },
  {
    "objectID": "week1.html#applications-in-urban-analytics",
    "href": "week1.html#applications-in-urban-analytics",
    "title": "1  Remote Sensing: To see the unseen",
    "section": "1.2 Applications in Urban Analytics",
    "text": "1.2 Applications in Urban Analytics\nRemote Sensing has many transformative applications in the realm of Urban Analytics. Think of it as an X-ray for cities, revealing hidden patterns and empowering informed decision-making by city planners, urban designers, and public officials to make swift and informed decisions to improve the lives of millions of urbanites.\nHere are some examples\n\nMapping Urban Growth: By tracking changes in the built environment over time, we can identify sprawling suburbs, monitor urban expansion, and plan for infrastructure needs. Remote sensing data are particularly rich sources to train classification models to extract features and identify unmapped communities.\n\n\n\n\nTransferable built-up area extraction (TBUAE) framework to map urbanised areas (Wang et al. 2021)\n\n\n\nPredicting Floods with Foresight: Analysing land cover and terrain, it anticipates where water will flow, safeguarding communities from harm, while also estimating potential damage. Here is an example of how insurance companies can make use of such data to assess risk and process claims.\n\n\n\n\nRemote sensing data can be used to estimate flood extent, and to derive individual risk level damage estimates (Schumann et al. 2023)\n\n\n\nEnergy Efficiency: Identifying heat island effects and understanding building energy consumption through thermal imaging empowers planners to design sustainable cities, and researchers to determine the best strategies to combat long term climate disasters.\n\n\n\n\nLeft: Average 2m air temperature at 23h (moment of max UHI) during all summer months (June-August) of the years 1987- 2016. Right: UrbCLIM output field downscaled to 30-m resolution showing the average daily maximum urban heat island (UHI) intensity for a part of the city of Amsterdam. (“DestinE for Human Heat Stress: ECMWF Use Case to Tackle Urban Heat Islands” n.d.)\n\n\nThis is just the beginning. Remote sensing is transforming the way we understand and manage our cities, paving the way for a healthier, smarter, and more sustainable urban future."
  },
  {
    "objectID": "week1.html#personal-reflections",
    "href": "week1.html#personal-reflections",
    "title": "1  Remote Sensing: To see the unseen",
    "section": "1.3 Personal Reflections",
    "text": "1.3 Personal Reflections\nMy first foray into the world of Remote Sensing was eye-opening, to say the least. For the uninitiated, it is easy to assume that remote sensing purely means orthophotographic satellite images that one might see using platforms such as Google Maps, i.e., as if the only thing that sensors do were to snap a simple photo of the planet like a phone camera.\nIn reality, it unlocks unseen depths of data beyond mere human perception, less high-resolution photographs and more spectral signatures, where each pixel holds a universe of information. Building a true-color composite from all these layers (so that our eyes can see) was like seeing the world through a brand-new lens.\nI am particularly excited to get started with Google Earth Engine later on as the primary gateway to access the wealth of remote sensing data and analytics with more ease in order to solve specific problems facing our world today.\n\n\n\n\n“DestinE for Human Heat Stress: ECMWF Use Case to Tackle Urban Heat Islands.” n.d. Accessed January 29, 2024. https://stories.ecmwf.int/destine-for-human-heat-stress-ecmwf-use-case-to-tackle-urban-heat-islands/.\n\n\nEarth Science Data Systems, NASA. 2019. “What Is Remote Sensing? | Earthdata.” Backgrounder. August 23, 2019. https://www.earthdata.nasa.gov/learn/backgrounders/remote-sensing.\n\n\n“Remote Sensing, Satellite Imaging Technology | Satellite Imaging Corp.” n.d. Accessed January 27, 2024. https://www.satimagingcorp.com/services/resources/characterization-of-satellite-remote-sensing-systems/.\n\n\nSchumann, Guy, Laura Giustarini, Angelica Tarpanelli, Ben Jarihani, and Sandro Martinis. 2023. “Flood Modeling and Prediction Using Earth Observation Data.” Surveys in Geophysics 44 (5): 1553–78. https://doi.org/10.1007/s10712-022-09751-y.\n\n\nWang, Haibo, Xueshuang Gong, Bingbing Wang, Chao Deng, and Qiong Cao. 2021. “Urban Development Analysis Using Built-up Area Maps Based on Multiple High-Resolution Satellite Data.” International Journal of Applied Earth Observation and Geoinformation 103 (December): 102500. https://doi.org/10.1016/j.jag.2021.102500."
  },
  {
    "objectID": "index.html#a-novices-guide-to-remote-sensing-and-earth-observation",
    "href": "index.html#a-novices-guide-to-remote-sensing-and-earth-observation",
    "title": "Viewed from Above: An Learning Diary on Earth Observation",
    "section": "A Novice’s guide to Remote Sensing and Earth Observation",
    "text": "A Novice’s guide to Remote Sensing and Earth Observation\nFrom another novice!\nIntro [TBD]"
  },
  {
    "objectID": "index.html#hello",
    "href": "index.html#hello",
    "title": "Viewed from Above: An Learning Diary on Earth Observation",
    "section": "Hello…",
    "text": "Hello…\n…from a novice to another!"
  },
  {
    "objectID": "week2.html#how-lidar-technology-is-enabling-self-driving-cars",
    "href": "week2.html#how-lidar-technology-is-enabling-self-driving-cars",
    "title": "2  Presenting LiDAR",
    "section": "How LiDAR technology is enabling self-driving cars",
    "text": "How LiDAR technology is enabling self-driving cars\nHaving understood how sensors work in theory, we can now shift our view to appreciate how one of them in particular, LiDAR (Light Detection and Ranging), is deployed in practice and can benefit us in an emerging sector: Autonomous Vehicles."
  },
  {
    "objectID": "week2.html#or-how-to-make-self-driving-cars-a-reality",
    "href": "week2.html#or-how-to-make-self-driving-cars-a-reality",
    "title": "2  A peek into the LiDAR technology",
    "section": "Or, how to make self-driving cars a reality",
    "text": "Or, how to make self-driving cars a reality\nHaving understood how sensors work in theory, we can now shift our view to appreciate how one of them in particular, LiDAR (Light Detection and Ranging), is deployed in practice and can benefit us in an emerging sector: Autonomous Vehicles.\n\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar-title\"&gt;Viewed from Above: An Learning Diary on Earth Observation&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar-title\"&gt;Viewed from Above: An Learning Diary on Earth Observation&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-next\"&gt;&lt;span class=\"chapter-number\"&gt;3&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Corrections and Enhancements&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-prev\"&gt;&lt;span class=\"chapter-number\"&gt;1&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Remote Sensing: To see the unseen&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar:/index.html\"&gt;Hello…&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar:quarto-sidebar-section-1\"&gt;&lt;strong&gt;Making sense of Remote Sensing&lt;/strong&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar:/week1.html\"&gt;&lt;span class=\"chapter-number\"&gt;1&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Remote Sensing: To see the unseen&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar:/week2.html\"&gt;&lt;span class=\"chapter-number\"&gt;2&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;A peek into the LiDAR technology&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar:/week3.html\"&gt;&lt;span class=\"chapter-number\"&gt;3&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Corrections and Enhancements&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar:quarto-sidebar-section-2\"&gt;&lt;strong&gt;Resources&lt;/strong&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar:/glossary.html\"&gt;Glossary: Demystifying jargons&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar:/references.html\"&gt;References&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-navbar-tools:https://github.com/hanukikanker/rs-diary\"&gt;https://github.com/hanukikanker/rs-diary&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-breadcrumbs-9136e573d908b18be8ad75a296650376\"&gt;&lt;strong&gt;Making sense of Remote Sensing&lt;/strong&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-breadcrumbs-d5ee1bb92bd6f3ef1319a012280c0252\"&gt;&lt;span class=\"chapter-number\"&gt;2&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;A peek into the LiDAR technology&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;div id=\"quarto-meta-markdown\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-metatitle\"&gt;Viewed from Above: An Learning Diary on Earth Observation - &lt;span class=\"chapter-number\"&gt;2&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;A peek into the LiDAR technology&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercardtitle\"&gt;Viewed from Above: An Learning Diary on Earth Observation - &lt;span class=\"chapter-number\"&gt;2&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;A peek into the LiDAR technology&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardtitle\"&gt;Viewed from Above: An Learning Diary on Earth Observation - &lt;span class=\"chapter-number\"&gt;2&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;A peek into the LiDAR technology&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-metasitename\"&gt;Viewed from Above: An Learning Diary on Earth Observation&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercarddesc\"&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardddesc\"&gt;&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;/section&gt;\n\n&lt;/main&gt; &lt;!-- /main --&gt;\n&lt;script id = \"quarto-html-after-body\" type=\"application/javascript\"&gt;\nwindow.document.addEventListener(\"DOMContentLoaded\", function (event) {\n  const toggleBodyColorMode = (bsSheetEl) =&gt; {\n    const mode = bsSheetEl.getAttribute(\"data-mode\");\n    const bodyEl = window.document.querySelector(\"body\");\n    if (mode === \"dark\") {\n      bodyEl.classList.add(\"quarto-dark\");\n      bodyEl.classList.remove(\"quarto-light\");\n    } else {\n      bodyEl.classList.add(\"quarto-light\");\n      bodyEl.classList.remove(\"quarto-dark\");\n    }\n  }\n  const toggleBodyColorPrimary = () =&gt; {\n    const bsSheetEl = window.document.querySelector(\"link#quarto-bootstrap\");\n    if (bsSheetEl) {\n      toggleBodyColorMode(bsSheetEl);\n    }\n  }\n  toggleBodyColorPrimary();  \n  const disableStylesheet = (stylesheets) =&gt; {\n    for (let i=0; i &lt; stylesheets.length; i++) {\n      const stylesheet = stylesheets[i];\n      stylesheet.rel = 'prefetch';\n    }\n  }\n  const enableStylesheet = (stylesheets) =&gt; {\n    for (let i=0; i &lt; stylesheets.length; i++) {\n      const stylesheet = stylesheets[i];\n      stylesheet.rel = 'stylesheet';\n    }\n  }\n  const manageTransitions = (selector, allowTransitions) =&gt; {\n    const els = window.document.querySelectorAll(selector);\n    for (let i=0; i &lt; els.length; i++) {\n      const el = els[i];\n      if (allowTransitions) {\n        el.classList.remove('notransition');\n      } else {\n        el.classList.add('notransition');\n      }\n    }\n  }\n  const toggleColorMode = (alternate) =&gt; {\n    // Switch the stylesheets\n    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');\n    manageTransitions('#quarto-margin-sidebar .nav-link', false);\n    if (alternate) {\n      enableStylesheet(alternateStylesheets);\n      for (const sheetNode of alternateStylesheets) {\n        if (sheetNode.id === \"quarto-bootstrap\") {\n          toggleBodyColorMode(sheetNode);\n        }\n      }\n    } else {\n      disableStylesheet(alternateStylesheets);\n      toggleBodyColorPrimary();\n    }\n    manageTransitions('#quarto-margin-sidebar .nav-link', true);\n    // Switch the toggles\n    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');\n    for (let i=0; i &lt; toggles.length; i++) {\n      const toggle = toggles[i];\n      if (toggle) {\n        if (alternate) {\n          toggle.classList.add(\"alternate\");     \n        } else {\n          toggle.classList.remove(\"alternate\");\n        }\n      }\n    }\n    // Hack to workaround the fact that safari doesn't\n    // properly recolor the scrollbar when toggling (#1455)\n    if (navigator.userAgent.indexOf('Safari') &gt; 0 && navigator.userAgent.indexOf('Chrome') == -1) {\n      manageTransitions(\"body\", false);\n      window.scrollTo(0, 1);\n      setTimeout(() =&gt; {\n        window.scrollTo(0, 0);\n        manageTransitions(\"body\", true);\n      }, 40);  \n    }\n  }\n  const isFileUrl = () =&gt; { \n    return window.location.protocol === 'file:';\n  }\n  const hasAlternateSentinel = () =&gt; {  \n    let styleSentinel = getColorSchemeSentinel();\n    if (styleSentinel !== null) {\n      return styleSentinel === \"alternate\";\n    } else {\n      return false;\n    }\n  }\n  const setStyleSentinel = (alternate) =&gt; {\n    const value = alternate ? \"alternate\" : \"default\";\n    if (!isFileUrl()) {\n      window.localStorage.setItem(\"quarto-color-scheme\", value);\n    } else {\n      localAlternateSentinel = value;\n    }\n  }\n  const getColorSchemeSentinel = () =&gt; {\n    if (!isFileUrl()) {\n      const storageValue = window.localStorage.getItem(\"quarto-color-scheme\");\n      return storageValue != null ? storageValue : localAlternateSentinel;\n    } else {\n      return localAlternateSentinel;\n    }\n  }\n  let localAlternateSentinel = 'default';\n  // Dark / light mode switch\n  window.quartoToggleColorScheme = () =&gt; {\n    // Read the current dark / light value \n    let toAlternate = !hasAlternateSentinel();\n    toggleColorMode(toAlternate);\n    setStyleSentinel(toAlternate);\n  };\n  // Ensure there is a toggle, if there isn't float one in the top right\n  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {\n    const a = window.document.createElement('a');\n    a.classList.add('top-right');\n    a.classList.add('quarto-color-scheme-toggle');\n    a.href = \"\";\n    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };\n    const i = window.document.createElement(\"i\");\n    i.classList.add('bi');\n    a.appendChild(i);\n    window.document.body.appendChild(a);\n  }\n  // Switch to dark mode if need be\n  if (hasAlternateSentinel()) {\n    toggleColorMode(true);\n  } else {\n    toggleColorMode(false);\n  }\n  const icon = \"\";\n  const anchorJS = new window.AnchorJS();\n  anchorJS.options = {\n    placement: 'right',\n    icon: icon\n  };\n  anchorJS.add('.anchored');\n  const isCodeAnnotation = (el) =&gt; {\n    for (const clz of el.classList) {\n      if (clz.startsWith('code-annotation-')) {                     \n        return true;\n      }\n    }\n    return false;\n  }\n  const clipboard = new window.ClipboardJS('.code-copy-button', {\n    text: function(trigger) {\n      const codeEl = trigger.previousElementSibling.cloneNode(true);\n      for (const childEl of codeEl.children) {\n        if (isCodeAnnotation(childEl)) {\n          childEl.remove();\n        }\n      }\n      return codeEl.innerText;\n    }\n  });\n  clipboard.on('success', function(e) {\n    // button target\n    const button = e.trigger;\n    // don't keep focus\n    button.blur();\n    // flash \"checked\"\n    button.classList.add('code-copy-button-checked');\n    var currentTitle = button.getAttribute(\"title\");\n    button.setAttribute(\"title\", \"Copied!\");\n    let tooltip;\n    if (window.bootstrap) {\n      button.setAttribute(\"data-bs-toggle\", \"tooltip\");\n      button.setAttribute(\"data-bs-placement\", \"left\");\n      button.setAttribute(\"data-bs-title\", \"Copied!\");\n      tooltip = new bootstrap.Tooltip(button, \n        { trigger: \"manual\", \n          customClass: \"code-copy-button-tooltip\",\n          offset: [0, -8]});\n      tooltip.show();    \n    }\n    setTimeout(function() {\n      if (tooltip) {\n        tooltip.hide();\n        button.removeAttribute(\"data-bs-title\");\n        button.removeAttribute(\"data-bs-toggle\");\n        button.removeAttribute(\"data-bs-placement\");\n      }\n      button.setAttribute(\"title\", currentTitle);\n      button.classList.remove('code-copy-button-checked');\n    }, 1000);\n    // clear code selection\n    e.clearSelection();\n  });\n  function tippyHover(el, contentFn) {\n    const config = {\n      allowHTML: true,\n      content: contentFn,\n      maxWidth: 500,\n      delay: 100,\n      arrow: false,\n      appendTo: function(el) {\n          return el.parentElement;\n      },\n      interactive: true,\n      interactiveBorder: 10,\n      theme: 'quarto',\n      placement: 'bottom-start'\n    };\n    window.tippy(el, config); \n  }\n  const noterefs = window.document.querySelectorAll('a[role=\"doc-noteref\"]');\n  for (var i=0; i&lt;noterefs.length; i++) {\n    const ref = noterefs[i];\n    tippyHover(ref, function() {\n      // use id or data attribute instead here\n      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');\n      try { href = new URL(href).hash; } catch {}\n      const id = href.replace(/^#\\/?/, \"\");\n      const note = window.document.getElementById(id);\n      return note.innerHTML;\n    });\n  }\n      let selectedAnnoteEl;\n      const selectorForAnnotation = ( cell, annotation) =&gt; {\n        let cellAttr = 'data-code-cell=\"' + cell + '\"';\n        let lineAttr = 'data-code-annotation=\"' +  annotation + '\"';\n        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';\n        return selector;\n      }\n      const selectCodeLines = (annoteEl) =&gt; {\n        const doc = window.document;\n        const targetCell = annoteEl.getAttribute(\"data-target-cell\");\n        const targetAnnotation = annoteEl.getAttribute(\"data-target-annotation\");\n        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));\n        const lines = annoteSpan.getAttribute(\"data-code-lines\").split(\",\");\n        const lineIds = lines.map((line) =&gt; {\n          return targetCell + \"-\" + line;\n        })\n        let top = null;\n        let height = null;\n        let parent = null;\n        if (lineIds.length &gt; 0) {\n            //compute the position of the single el (top and bottom and make a div)\n            const el = window.document.getElementById(lineIds[0]);\n            top = el.offsetTop;\n            height = el.offsetHeight;\n            parent = el.parentElement.parentElement;\n          if (lineIds.length &gt; 1) {\n            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);\n            const bottom = lastEl.offsetTop + lastEl.offsetHeight;\n            height = bottom - top;\n          }\n          if (top !== null && height !== null && parent !== null) {\n            // cook up a div (if necessary) and position it \n            let div = window.document.getElementById(\"code-annotation-line-highlight\");\n            if (div === null) {\n              div = window.document.createElement(\"div\");\n              div.setAttribute(\"id\", \"code-annotation-line-highlight\");\n              div.style.position = 'absolute';\n              parent.appendChild(div);\n            }\n            div.style.top = top - 2 + \"px\";\n            div.style.height = height + 4 + \"px\";\n            let gutterDiv = window.document.getElementById(\"code-annotation-line-highlight-gutter\");\n            if (gutterDiv === null) {\n              gutterDiv = window.document.createElement(\"div\");\n              gutterDiv.setAttribute(\"id\", \"code-annotation-line-highlight-gutter\");\n              gutterDiv.style.position = 'absolute';\n              const codeCell = window.document.getElementById(targetCell);\n              const gutter = codeCell.querySelector('.code-annotation-gutter');\n              gutter.appendChild(gutterDiv);\n            }\n            gutterDiv.style.top = top - 2 + \"px\";\n            gutterDiv.style.height = height + 4 + \"px\";\n          }\n          selectedAnnoteEl = annoteEl;\n        }\n      };\n      const unselectCodeLines = () =&gt; {\n        const elementsIds = [\"code-annotation-line-highlight\", \"code-annotation-line-highlight-gutter\"];\n        elementsIds.forEach((elId) =&gt; {\n          const div = window.document.getElementById(elId);\n          if (div) {\n            div.remove();\n          }\n        });\n        selectedAnnoteEl = undefined;\n      };\n      // Attach click handler to the DT\n      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');\n      for (const annoteDlNode of annoteDls) {\n        annoteDlNode.addEventListener('click', (event) =&gt; {\n          const clickedEl = event.target;\n          if (clickedEl !== selectedAnnoteEl) {\n            unselectCodeLines();\n            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');\n            if (activeEl) {\n              activeEl.classList.remove('code-annotation-active');\n            }\n            selectCodeLines(clickedEl);\n            clickedEl.classList.add('code-annotation-active');\n          } else {\n            // Unselect the line\n            unselectCodeLines();\n            clickedEl.classList.remove('code-annotation-active');\n          }\n        });\n      }\n  const findCites = (el) =&gt; {\n    const parentEl = el.parentElement;\n    if (parentEl) {\n      const cites = parentEl.dataset.cites;\n      if (cites) {\n        return {\n          el,\n          cites: cites.split(' ')\n        };\n      } else {\n        return findCites(el.parentElement)\n      }\n    } else {\n      return undefined;\n    }\n  };\n  var bibliorefs = window.document.querySelectorAll('a[role=\"doc-biblioref\"]');\n  for (var i=0; i&lt;bibliorefs.length; i++) {\n    const ref = bibliorefs[i];\n    const citeInfo = findCites(ref);\n    if (citeInfo) {\n      tippyHover(citeInfo.el, function() {\n        var popup = window.document.createElement('div');\n        citeInfo.cites.forEach(function(cite) {\n          var citeDiv = window.document.createElement('div');\n          citeDiv.classList.add('hanging-indent');\n          citeDiv.classList.add('csl-entry');\n          var biblioDiv = window.document.getElementById('ref-' + cite);\n          if (biblioDiv) {\n            citeDiv.innerHTML = biblioDiv.innerHTML;\n          }\n          popup.appendChild(citeDiv);\n        });\n        return popup.innerHTML;\n      });\n    }\n  }\n});\n&lt;/script&gt;\n&lt;nav class=\"page-navigation\"&gt;\n  &lt;div class=\"nav-page nav-page-previous\"&gt;\n      &lt;a  href=\"/week1.html\" class=\"pagination-link\"&gt;\n        &lt;i class=\"bi bi-arrow-left-short\"&gt;&lt;/i&gt; &lt;span class=\"nav-page-text\"&gt;&lt;span class='chapter-number'&gt;1&lt;/span&gt;  &lt;span class='chapter-title'&gt;Remote Sensing: To see the unseen&lt;/span&gt;&lt;/span&gt;\n      &lt;/a&gt;          \n  &lt;/div&gt;\n  &lt;div class=\"nav-page nav-page-next\"&gt;\n      &lt;a  href=\"/week3.html\" class=\"pagination-link\"&gt;\n        &lt;span class=\"nav-page-text\"&gt;&lt;span class='chapter-number'&gt;3&lt;/span&gt;  &lt;span class='chapter-title'&gt;Corrections and Enhancements&lt;/span&gt;&lt;/span&gt; &lt;i class=\"bi bi-arrow-right-short\"&gt;&lt;/i&gt;\n      &lt;/a&gt;\n  &lt;/div&gt;\n&lt;/nav&gt;\n&lt;/div&gt; &lt;!-- /content --&gt;\n\n&lt;/body&gt;\n\n&lt;/html&gt;"
  },
  {
    "objectID": "week3.html#corrections",
    "href": "week3.html#corrections",
    "title": "3  Corrections and Enhancements",
    "section": "3.1 Corrections",
    "text": "3.1 Corrections\nRaw data coming from sensors are rarely immediately usable without being corrected for various interferences and effects. Here is\n\nGeometric correction\nAtmospheric correction\nTopographic correction\nRadiometric correction\n\n\n3.1.1 Radiometric and Atmospheric corrections\nThese processes describe translating raw light data from the sensor into ‘true’ information on the surface’s reflectance property, without interference from the light source and the atmosphere.\n\nRadiometric calibration is the conversion from raw Digital Number to Spectral Radiance via a linear transformation \\(L_λ=Bias+(Gain∗DN)\\). Radiance most often has units of watt/(steradian/square meter)\nAtmospheric correction involves the next step:\n\nTOA Radiance-to-Reflectance correction removes effects of the light source (e.g. the sun) by calibrating radiation going down (irradiance) and up (radiance). TOA Reflectance still has the effect of the atmosphere and the surface material. If irradiance = radiance, we call this hemispheric reflectance\n\nTOA-to-BOA Reflectance correction removes effects of the atmospheric conditions, leaving us with just data on the surface materials. If shadows and directional effects on reflectance have been dealt with, we get what is called true reflectance, if not then it is called apparent reflectance.\n\n\nAtmospheric correction deserves out attention considering the effect of atmospheric scattering on the final results. Absorption and scattering create the haze which reduces the contrast, and can creates the “adjacency effect”, whereby radiance from pixels nearby mixed into pixel of interest. There are several types of atmospheric correction:\n\nRelative: normalise intensities of different bands within a single image, or from many dates to one date. This can be done via Dark Object Subtraction (DOS), Empirical Line Correction, or Pseudo-invariant Features (PIFs)\nAbsolute: change digital brightness values into scaled surface reflectance. We can then compare these scaled surface reflectance values across the planet, through atmospheric radiative transfer models (i.e. summer, tropical). To perform this correction, we also need local atmospheric visibility and image altitude. There are many tools for this purpose:\n\nPaid: ACORN, FLAASH, QUAC, ATCOR\nFree: SMAC, Orfeo Toolbox\n\n\n\n\n\nExample of LEDAPS atmospheric correction. (a) Top-of-atmosphere (TOA) reflectance composite (bands 3,2,1) for Landsat-7 ETM+ image of San Francisco Bay (July 7, 1999); (b) Surface reflectance composite.\n\n\nFinally, atmospheric correction to obtain true reflectance is not always necessary, for example, for classification of a single image, working on composite images, etc.\n\n\n3.1.2 Geometric and topographic corrections\nSubsets of Georectification"
  },
  {
    "objectID": "week3.html#in-practice",
    "href": "week3.html#in-practice",
    "title": "3  Corrections and Enhancements",
    "section": "3.2 In practice",
    "text": "3.2 In practice"
  },
  {
    "objectID": "week3.html#reflections",
    "href": "week3.html#reflections",
    "title": "3  Corrections and Enhancements",
    "section": "3.3 Reflections",
    "text": "3.3 Reflections"
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary: Demystifying jargons",
    "section": "",
    "text": "Digital Number (DN) : raw brightness data captured by sensors without having units for efficient storage\nRadiance : how much light the instrument sees in meaningful units but still have effects of light source, atmosphere and surface material. Also known as Top of Atmosphere (TOA) radiance\nReflectance : a property of a material, it is a ratio of the amount of light leaving a target to the amount of light striking it. Its calculation is performed on radiance corrected data. Typically this comes as surface (BOA) reflectance but can also be Top of Atmosphere (TOA) reflectance if the atmospheric effect on radiance has not been accounted for.\nRadiometric calibration : the process from converting DN to Radiance Lλ=Bias+(Gain∗DN)\nDark object subtraction (DOS) : A relative Atmospheric correction technique. Searches each band for the darkest value then subtracts that from each pixel. This is appropriate when the visible bands have increased scattering vs. longer wavelengths. Also known as, Histogram Adjustment\n\n\n\nPseudo-invariant Features (PIFs) : A relative Atmospheric correction technique. Assume brightness pixels linearly related to a base image (linear regression) and adjust the image based on the regression result\nAtmospheric radiative transfer models."
  }
]