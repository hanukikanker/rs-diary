[
  {
    "objectID": "index.html#hello",
    "href": "index.html#hello",
    "title": "Viewed from Above: An Learning Diary on Remote Sensing & Earth Observation",
    "section": "Hello…",
    "text": "Hello…\n…from a novice to another!"
  },
  {
    "objectID": "week1.html#in-summary",
    "href": "week1.html#in-summary",
    "title": "1  Remote Sensing: To see the unseen",
    "section": "1.1 In summary…",
    "text": "1.1 In summary…\nRemote Sensing is the use of satellites, planes, drones, etc. as our aerial eyes, piecing together a portrait of our planet through light and data and revolutionising the way we understand and interact with it.\nSensors, well, ‘sense’ Earth in two ways: Some passively listen to sunlight reflected off our planet’s surface (i.e., passive sensors), while others actively send their own signals and capture the echoes (i.e., active sensors). By that definition, the human eye is a type of passive sensors!\n\n\n\nActive vs Passive Remote Sensing (Earth Science Data Systems 2019).\n\n\nThese sensors interpret a fascinating language of light, both invisible and visible, known as the electromagnetic spectrum.\n\n\n\nThe Electromagnetic Spectrum (EMS). Credit: NASA Science\n\n\nElectromagnetic radiation moves as waves as perpendicular electric and magnetic field with a wavelength: λ = c/v where:\n\nλ = wavelength, distance between two crests\nc = velocity of light 3 x 108 m/sec\nv = frequency, rate of oscillation (full oscillations in a time unit)\n\nDifferent materials reflect unique wavelengths in this spectrum, allowing us to identify them, like decoding DNA! More on this later…\n\n\n\nWavelength vs. Oscillation\n\n\nBut information that sensors receive isn’t just about color. Remote sensing data has its own “resolution” recipe, encompassing:\n\nSpectral: How EMS bands (i.e., the range within the EMS) the sensor can hear, revealing more detail with each additional band.\nSpatial: The size of each pixel in the image, ranging from centimeters to kilometers, offering varying levels of detail.\nTemporal: How often the sensor revisits the same area, providing a dynamic view of changes over time.\nRadiometric: The range of brightness levels captured, painting a vibrant and accurate picture.\n\nIn reality, depending on the purpose, each sensor is equip to have better resolution of one type over the other. For example, sensors with a high spatial resolution of 5m (i.e., each pixel is 10x10 on the ground) will have lower spectral resolution (i.e., capturing only narrow range of the EMS) (“Remote Sensing, Satellite Imaging Technology | Satellite Imaging Corp” n.d.)"
  },
  {
    "objectID": "week1.html#in-practice",
    "href": "week1.html#in-practice",
    "title": "1  Remote Sensing: To see the unseen",
    "section": "1.2 In practice…",
    "text": "1.2 In practice…\nRemote Sensing has many transformative applications in the realm of Urban Analytics. Think of it as an X-ray for cities, revealing hidden patterns and empowering informed decision-making by city planners, urban designers, and public officials to make swift and informed decisions to improve the lives of millions of urbanites.\nHere are some examples\n\nMapping Urban Growth: By tracking changes in the built environment over time, we can identify sprawling suburbs, monitor urban expansion, and plan for infrastructure needs. Remote sensing data are particularly rich sources to train classification models to extract features and identify unmapped communities.\n\n\n\n\nTransferable built-up area extraction (TBUAE) framework to map urbanised areas (Wang et al. 2021)\n\n\n\nPredicting Floods with Foresight: Analysing land cover and terrain, it anticipates where water will flow, safeguarding communities from harm, while also estimating potential damage. Here is an example of how insurance companies can make use of such data to assess risk and process claims.\n\n\n\n\nRemote sensing data can be used to estimate flood extent, and to derive individual risk level damage estimates (Schumann et al. 2023)\n\n\n\nEnergy Efficiency: Identifying heat island effects and understanding building energy consumption through thermal imaging empowers planners to design sustainable cities, and researchers to determine the best strategies to combat long term climate disasters.\n\n\n\n\nLeft: Average 2m air temperature at 23h (moment of max UHI) during all summer months (June-August) of the years 1987- 2016. Right: UrbCLIM output field downscaled to 30-m resolution showing the average daily maximum urban heat island (UHI) intensity for a part of the city of Amsterdam. (“DestinE for Human Heat Stress: ECMWF Use Case to Tackle Urban Heat Islands” n.d.)\n\n\nThis is just the beginning. Remote sensing is transforming the way we understand and manage our cities, paving the way for a healthier, smarter, and more sustainable urban future."
  },
  {
    "objectID": "week1.html#in-short",
    "href": "week1.html#in-short",
    "title": "1  Remote Sensing: To see the unseen",
    "section": "1.3 In short…",
    "text": "1.3 In short…\nMy first foray into the world of Remote Sensing was eye-opening, to say the least. For the uninitiated, it is easy to assume that remote sensing purely means orthophotographic satellite images that one might see using platforms such as Google Maps, i.e., as if the only thing that sensors do were to snap a simple photo of the planet like a phone camera.\nIn reality, it unlocks unseen depths of data beyond mere human perception, less high-resolution photographs and more spectral signatures, where each pixel holds a universe of information. Building a true-color composite from all these layers (so that our eyes can see) was like seeing the world through a brand-new lens.\nI am particularly excited to get started with Google Earth Engine later on as the primary gateway to access the wealth of remote sensing data and analytics with more ease in order to solve specific problems facing our world today.\n\n\n\n\n“DestinE for Human Heat Stress: ECMWF Use Case to Tackle Urban Heat Islands.” n.d. Accessed January 29, 2024. https://stories.ecmwf.int/destine-for-human-heat-stress-ecmwf-use-case-to-tackle-urban-heat-islands/.\n\n\nEarth Science Data Systems, NASA. 2019. “What Is Remote Sensing? | Earthdata.” Backgrounder. August 23, 2019. https://www.earthdata.nasa.gov/learn/backgrounders/remote-sensing.\n\n\n“Remote Sensing, Satellite Imaging Technology | Satellite Imaging Corp.” n.d. Accessed January 27, 2024. https://www.satimagingcorp.com/services/resources/characterization-of-satellite-remote-sensing-systems/.\n\n\nSchumann, Guy, Laura Giustarini, Angelica Tarpanelli, Ben Jarihani, and Sandro Martinis. 2023. “Flood Modeling and Prediction Using Earth Observation Data.” Surveys in Geophysics 44 (5): 1553–78. https://doi.org/10.1007/s10712-022-09751-y.\n\n\nWang, Haibo, Xueshuang Gong, Bingbing Wang, Chao Deng, and Qiong Cao. 2021. “Urban Development Analysis Using Built-up Area Maps Based on Multiple High-Resolution Satellite Data.” International Journal of Applied Earth Observation and Geoinformation 103 (December): 102500. https://doi.org/10.1016/j.jag.2021.102500."
  },
  {
    "objectID": "week2.html#or-how-to-make-self-driving-cars-a-reality",
    "href": "week2.html#or-how-to-make-self-driving-cars-a-reality",
    "title": "2  A peek into the LiDAR technology",
    "section": "Or, how to make self-driving cars a reality",
    "text": "Or, how to make self-driving cars a reality\nHaving understood how sensors work in theory, we can now shift our view to appreciate how one of them in particular, LiDAR (Light Detection and Ranging), is deployed in practice and can benefit us in an emerging sector: Autonomous Vehicles.\n\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar-title\"&gt;Viewed from Above: An Learning Diary on Remote Sensing &amp; Earth Observation&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-navbar-title\"&gt;Viewed from Above: An Learning Diary on Remote Sensing &amp; Earth Observation&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-next\"&gt;&lt;span class=\"chapter-number\"&gt;3&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Corrections and Enhancements&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-prev\"&gt;&lt;span class=\"chapter-number\"&gt;1&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Remote Sensing: To see the unseen&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar:/index.html\"&gt;Hello…&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar:quarto-sidebar-section-1\"&gt;&lt;strong&gt;Making sense of Remote Sensing&lt;/strong&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar:/week1.html\"&gt;&lt;span class=\"chapter-number\"&gt;1&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Remote Sensing: To see the unseen&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar:/week2.html\"&gt;&lt;span class=\"chapter-number\"&gt;2&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;A peek into the LiDAR technology&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar:/week3.html\"&gt;&lt;span class=\"chapter-number\"&gt;3&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;Corrections and Enhancements&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar:quarto-sidebar-section-2\"&gt;&lt;strong&gt;Resources&lt;/strong&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar:/glossary.html\"&gt;Glossary: Demystifying jargons&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-int-sidebar:/references.html\"&gt;References&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-navbar-tools:https://github.com/hanukikanker/rs-diary\"&gt;https://github.com/hanukikanker/rs-diary&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-breadcrumbs-9136e573d908b18be8ad75a296650376\"&gt;&lt;strong&gt;Making sense of Remote Sensing&lt;/strong&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-breadcrumbs-d5ee1bb92bd6f3ef1319a012280c0252\"&gt;&lt;span class=\"chapter-number\"&gt;2&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;A peek into the LiDAR technology&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;div id=\"quarto-meta-markdown\" class=\"hidden\"&gt;\n&lt;p&gt;&lt;span class=\"hidden\" data-render-id=\"quarto-metatitle\"&gt;Viewed from Above: An Learning Diary on Remote Sensing &amp; Earth Observation - &lt;span class=\"chapter-number\"&gt;2&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;A peek into the LiDAR technology&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercardtitle\"&gt;Viewed from Above: An Learning Diary on Remote Sensing &amp; Earth Observation - &lt;span class=\"chapter-number\"&gt;2&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;A peek into the LiDAR technology&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardtitle\"&gt;Viewed from Above: An Learning Diary on Remote Sensing &amp; Earth Observation - &lt;span class=\"chapter-number\"&gt;2&lt;/span&gt;  &lt;span class=\"chapter-title\"&gt;A peek into the LiDAR technology&lt;/span&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-metasitename\"&gt;Viewed from Above: An Learning Diary on Remote Sensing &amp; Earth Observation&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-twittercarddesc\"&gt;&lt;/span&gt; &lt;span class=\"hidden\" data-render-id=\"quarto-ogcardddesc\"&gt;&lt;/span&gt;&lt;/p&gt;\n&lt;/div&gt;\n&lt;/section&gt;\n\n&lt;/main&gt; &lt;!-- /main --&gt;\n&lt;script id = \"quarto-html-after-body\" type=\"application/javascript\"&gt;\nwindow.document.addEventListener(\"DOMContentLoaded\", function (event) {\n  const toggleBodyColorMode = (bsSheetEl) =&gt; {\n    const mode = bsSheetEl.getAttribute(\"data-mode\");\n    const bodyEl = window.document.querySelector(\"body\");\n    if (mode === \"dark\") {\n      bodyEl.classList.add(\"quarto-dark\");\n      bodyEl.classList.remove(\"quarto-light\");\n    } else {\n      bodyEl.classList.add(\"quarto-light\");\n      bodyEl.classList.remove(\"quarto-dark\");\n    }\n  }\n  const toggleBodyColorPrimary = () =&gt; {\n    const bsSheetEl = window.document.querySelector(\"link#quarto-bootstrap\");\n    if (bsSheetEl) {\n      toggleBodyColorMode(bsSheetEl);\n    }\n  }\n  toggleBodyColorPrimary();  \n  const disableStylesheet = (stylesheets) =&gt; {\n    for (let i=0; i &lt; stylesheets.length; i++) {\n      const stylesheet = stylesheets[i];\n      stylesheet.rel = 'prefetch';\n    }\n  }\n  const enableStylesheet = (stylesheets) =&gt; {\n    for (let i=0; i &lt; stylesheets.length; i++) {\n      const stylesheet = stylesheets[i];\n      stylesheet.rel = 'stylesheet';\n    }\n  }\n  const manageTransitions = (selector, allowTransitions) =&gt; {\n    const els = window.document.querySelectorAll(selector);\n    for (let i=0; i &lt; els.length; i++) {\n      const el = els[i];\n      if (allowTransitions) {\n        el.classList.remove('notransition');\n      } else {\n        el.classList.add('notransition');\n      }\n    }\n  }\n  const toggleColorMode = (alternate) =&gt; {\n    // Switch the stylesheets\n    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');\n    manageTransitions('#quarto-margin-sidebar .nav-link', false);\n    if (alternate) {\n      enableStylesheet(alternateStylesheets);\n      for (const sheetNode of alternateStylesheets) {\n        if (sheetNode.id === \"quarto-bootstrap\") {\n          toggleBodyColorMode(sheetNode);\n        }\n      }\n    } else {\n      disableStylesheet(alternateStylesheets);\n      toggleBodyColorPrimary();\n    }\n    manageTransitions('#quarto-margin-sidebar .nav-link', true);\n    // Switch the toggles\n    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');\n    for (let i=0; i &lt; toggles.length; i++) {\n      const toggle = toggles[i];\n      if (toggle) {\n        if (alternate) {\n          toggle.classList.add(\"alternate\");     \n        } else {\n          toggle.classList.remove(\"alternate\");\n        }\n      }\n    }\n    // Hack to workaround the fact that safari doesn't\n    // properly recolor the scrollbar when toggling (#1455)\n    if (navigator.userAgent.indexOf('Safari') &gt; 0 && navigator.userAgent.indexOf('Chrome') == -1) {\n      manageTransitions(\"body\", false);\n      window.scrollTo(0, 1);\n      setTimeout(() =&gt; {\n        window.scrollTo(0, 0);\n        manageTransitions(\"body\", true);\n      }, 40);  \n    }\n  }\n  const isFileUrl = () =&gt; { \n    return window.location.protocol === 'file:';\n  }\n  const hasAlternateSentinel = () =&gt; {  \n    let styleSentinel = getColorSchemeSentinel();\n    if (styleSentinel !== null) {\n      return styleSentinel === \"alternate\";\n    } else {\n      return false;\n    }\n  }\n  const setStyleSentinel = (alternate) =&gt; {\n    const value = alternate ? \"alternate\" : \"default\";\n    if (!isFileUrl()) {\n      window.localStorage.setItem(\"quarto-color-scheme\", value);\n    } else {\n      localAlternateSentinel = value;\n    }\n  }\n  const getColorSchemeSentinel = () =&gt; {\n    if (!isFileUrl()) {\n      const storageValue = window.localStorage.getItem(\"quarto-color-scheme\");\n      return storageValue != null ? storageValue : localAlternateSentinel;\n    } else {\n      return localAlternateSentinel;\n    }\n  }\n  let localAlternateSentinel = 'default';\n  // Dark / light mode switch\n  window.quartoToggleColorScheme = () =&gt; {\n    // Read the current dark / light value \n    let toAlternate = !hasAlternateSentinel();\n    toggleColorMode(toAlternate);\n    setStyleSentinel(toAlternate);\n  };\n  // Ensure there is a toggle, if there isn't float one in the top right\n  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {\n    const a = window.document.createElement('a');\n    a.classList.add('top-right');\n    a.classList.add('quarto-color-scheme-toggle');\n    a.href = \"\";\n    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };\n    const i = window.document.createElement(\"i\");\n    i.classList.add('bi');\n    a.appendChild(i);\n    window.document.body.appendChild(a);\n  }\n  // Switch to dark mode if need be\n  if (hasAlternateSentinel()) {\n    toggleColorMode(true);\n  } else {\n    toggleColorMode(false);\n  }\n  const icon = \"\";\n  const anchorJS = new window.AnchorJS();\n  anchorJS.options = {\n    placement: 'right',\n    icon: icon\n  };\n  anchorJS.add('.anchored');\n  const isCodeAnnotation = (el) =&gt; {\n    for (const clz of el.classList) {\n      if (clz.startsWith('code-annotation-')) {                     \n        return true;\n      }\n    }\n    return false;\n  }\n  const clipboard = new window.ClipboardJS('.code-copy-button', {\n    text: function(trigger) {\n      const codeEl = trigger.previousElementSibling.cloneNode(true);\n      for (const childEl of codeEl.children) {\n        if (isCodeAnnotation(childEl)) {\n          childEl.remove();\n        }\n      }\n      return codeEl.innerText;\n    }\n  });\n  clipboard.on('success', function(e) {\n    // button target\n    const button = e.trigger;\n    // don't keep focus\n    button.blur();\n    // flash \"checked\"\n    button.classList.add('code-copy-button-checked');\n    var currentTitle = button.getAttribute(\"title\");\n    button.setAttribute(\"title\", \"Copied!\");\n    let tooltip;\n    if (window.bootstrap) {\n      button.setAttribute(\"data-bs-toggle\", \"tooltip\");\n      button.setAttribute(\"data-bs-placement\", \"left\");\n      button.setAttribute(\"data-bs-title\", \"Copied!\");\n      tooltip = new bootstrap.Tooltip(button, \n        { trigger: \"manual\", \n          customClass: \"code-copy-button-tooltip\",\n          offset: [0, -8]});\n      tooltip.show();    \n    }\n    setTimeout(function() {\n      if (tooltip) {\n        tooltip.hide();\n        button.removeAttribute(\"data-bs-title\");\n        button.removeAttribute(\"data-bs-toggle\");\n        button.removeAttribute(\"data-bs-placement\");\n      }\n      button.setAttribute(\"title\", currentTitle);\n      button.classList.remove('code-copy-button-checked');\n    }, 1000);\n    // clear code selection\n    e.clearSelection();\n  });\n  function tippyHover(el, contentFn) {\n    const config = {\n      allowHTML: true,\n      content: contentFn,\n      maxWidth: 500,\n      delay: 100,\n      arrow: false,\n      appendTo: function(el) {\n          return el.parentElement;\n      },\n      interactive: true,\n      interactiveBorder: 10,\n      theme: 'quarto',\n      placement: 'bottom-start'\n    };\n    window.tippy(el, config); \n  }\n  const noterefs = window.document.querySelectorAll('a[role=\"doc-noteref\"]');\n  for (var i=0; i&lt;noterefs.length; i++) {\n    const ref = noterefs[i];\n    tippyHover(ref, function() {\n      // use id or data attribute instead here\n      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');\n      try { href = new URL(href).hash; } catch {}\n      const id = href.replace(/^#\\/?/, \"\");\n      const note = window.document.getElementById(id);\n      return note.innerHTML;\n    });\n  }\n      let selectedAnnoteEl;\n      const selectorForAnnotation = ( cell, annotation) =&gt; {\n        let cellAttr = 'data-code-cell=\"' + cell + '\"';\n        let lineAttr = 'data-code-annotation=\"' +  annotation + '\"';\n        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';\n        return selector;\n      }\n      const selectCodeLines = (annoteEl) =&gt; {\n        const doc = window.document;\n        const targetCell = annoteEl.getAttribute(\"data-target-cell\");\n        const targetAnnotation = annoteEl.getAttribute(\"data-target-annotation\");\n        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));\n        const lines = annoteSpan.getAttribute(\"data-code-lines\").split(\",\");\n        const lineIds = lines.map((line) =&gt; {\n          return targetCell + \"-\" + line;\n        })\n        let top = null;\n        let height = null;\n        let parent = null;\n        if (lineIds.length &gt; 0) {\n            //compute the position of the single el (top and bottom and make a div)\n            const el = window.document.getElementById(lineIds[0]);\n            top = el.offsetTop;\n            height = el.offsetHeight;\n            parent = el.parentElement.parentElement;\n          if (lineIds.length &gt; 1) {\n            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);\n            const bottom = lastEl.offsetTop + lastEl.offsetHeight;\n            height = bottom - top;\n          }\n          if (top !== null && height !== null && parent !== null) {\n            // cook up a div (if necessary) and position it \n            let div = window.document.getElementById(\"code-annotation-line-highlight\");\n            if (div === null) {\n              div = window.document.createElement(\"div\");\n              div.setAttribute(\"id\", \"code-annotation-line-highlight\");\n              div.style.position = 'absolute';\n              parent.appendChild(div);\n            }\n            div.style.top = top - 2 + \"px\";\n            div.style.height = height + 4 + \"px\";\n            let gutterDiv = window.document.getElementById(\"code-annotation-line-highlight-gutter\");\n            if (gutterDiv === null) {\n              gutterDiv = window.document.createElement(\"div\");\n              gutterDiv.setAttribute(\"id\", \"code-annotation-line-highlight-gutter\");\n              gutterDiv.style.position = 'absolute';\n              const codeCell = window.document.getElementById(targetCell);\n              const gutter = codeCell.querySelector('.code-annotation-gutter');\n              gutter.appendChild(gutterDiv);\n            }\n            gutterDiv.style.top = top - 2 + \"px\";\n            gutterDiv.style.height = height + 4 + \"px\";\n          }\n          selectedAnnoteEl = annoteEl;\n        }\n      };\n      const unselectCodeLines = () =&gt; {\n        const elementsIds = [\"code-annotation-line-highlight\", \"code-annotation-line-highlight-gutter\"];\n        elementsIds.forEach((elId) =&gt; {\n          const div = window.document.getElementById(elId);\n          if (div) {\n            div.remove();\n          }\n        });\n        selectedAnnoteEl = undefined;\n      };\n      // Attach click handler to the DT\n      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');\n      for (const annoteDlNode of annoteDls) {\n        annoteDlNode.addEventListener('click', (event) =&gt; {\n          const clickedEl = event.target;\n          if (clickedEl !== selectedAnnoteEl) {\n            unselectCodeLines();\n            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');\n            if (activeEl) {\n              activeEl.classList.remove('code-annotation-active');\n            }\n            selectCodeLines(clickedEl);\n            clickedEl.classList.add('code-annotation-active');\n          } else {\n            // Unselect the line\n            unselectCodeLines();\n            clickedEl.classList.remove('code-annotation-active');\n          }\n        });\n      }\n  const findCites = (el) =&gt; {\n    const parentEl = el.parentElement;\n    if (parentEl) {\n      const cites = parentEl.dataset.cites;\n      if (cites) {\n        return {\n          el,\n          cites: cites.split(' ')\n        };\n      } else {\n        return findCites(el.parentElement)\n      }\n    } else {\n      return undefined;\n    }\n  };\n  var bibliorefs = window.document.querySelectorAll('a[role=\"doc-biblioref\"]');\n  for (var i=0; i&lt;bibliorefs.length; i++) {\n    const ref = bibliorefs[i];\n    const citeInfo = findCites(ref);\n    if (citeInfo) {\n      tippyHover(citeInfo.el, function() {\n        var popup = window.document.createElement('div');\n        citeInfo.cites.forEach(function(cite) {\n          var citeDiv = window.document.createElement('div');\n          citeDiv.classList.add('hanging-indent');\n          citeDiv.classList.add('csl-entry');\n          var biblioDiv = window.document.getElementById('ref-' + cite);\n          if (biblioDiv) {\n            citeDiv.innerHTML = biblioDiv.innerHTML;\n          }\n          popup.appendChild(citeDiv);\n        });\n        return popup.innerHTML;\n      });\n    }\n  }\n});\n&lt;/script&gt;\n&lt;nav class=\"page-navigation\"&gt;\n  &lt;div class=\"nav-page nav-page-previous\"&gt;\n      &lt;a  href=\"/week1.html\" class=\"pagination-link\"&gt;\n        &lt;i class=\"bi bi-arrow-left-short\"&gt;&lt;/i&gt; &lt;span class=\"nav-page-text\"&gt;&lt;span class='chapter-number'&gt;1&lt;/span&gt;  &lt;span class='chapter-title'&gt;Remote Sensing: To see the unseen&lt;/span&gt;&lt;/span&gt;\n      &lt;/a&gt;          \n  &lt;/div&gt;\n  &lt;div class=\"nav-page nav-page-next\"&gt;\n      &lt;a  href=\"/week3.html\" class=\"pagination-link\"&gt;\n        &lt;span class=\"nav-page-text\"&gt;&lt;span class='chapter-number'&gt;3&lt;/span&gt;  &lt;span class='chapter-title'&gt;Corrections and Enhancements&lt;/span&gt;&lt;/span&gt; &lt;i class=\"bi bi-arrow-right-short\"&gt;&lt;/i&gt;\n      &lt;/a&gt;\n  &lt;/div&gt;\n&lt;/nav&gt;\n&lt;/div&gt; &lt;!-- /content --&gt;\n\n&lt;/body&gt;\n\n&lt;/html&gt;"
  },
  {
    "objectID": "week3.html#in-summary",
    "href": "week3.html#in-summary",
    "title": "3  Corrections and Enhancements",
    "section": "3.1 In summary…",
    "text": "3.1 In summary…\nRaw data coming from sensors are rarely immediately usable without being corrected for various interferences and effects. Here is\n\nGeometric correction\nAtmospheric correction\nTopographic correction\nRadiometric correction\n\n\n3.1.1 Radiometric and Atmospheric corrections\nThese processes describe translating raw light data from the sensor into ‘true’ information on the surface’s reflectance property, without interference from the light source and the atmosphere.\n\nRadiometric calibration is the conversion from raw Digital Number to Spectral Radiance via a linear transformation \\(L_λ=Bias+(Gain∗DN)\\). Radiance most often has units of watt/(steradian/square meter)\nAtmospheric correction involves the next step:\n\nTOA Radiance-to-Reflectance correction removes effects of the light source (e.g. the sun) by calibrating radiation going down (irradiance) and up (radiance). TOA Reflectance still has the effect of the atmosphere and the surface material. If irradiance = radiance, we call this hemispheric reflectance\n\nTOA-to-BOA Reflectance correction removes effects of the atmospheric conditions, leaving us with just data on the surface materials. If shadows and directional effects on reflectance have been dealt with, we get what is called true reflectance, if not then it is called apparent reflectance.\n\n\nAtmospheric correction deserves out attention considering the effect of atmospheric scattering on the final results. Absorption and scattering create the haze which reduces the contrast, and can creates the “adjacency effect”, whereby radiance from pixels nearby mixed into pixel of interest. There are several types of atmospheric correction:\n\nRelative: normalise intensities of different bands within a single image, or from many dates to one date. This can be done via Dark Object Subtraction (DOS), Empirical Line Correction, or Pseudo-invariant Features (PIFs)\nAbsolute: change digital brightness values into scaled surface reflectance. We can then compare these scaled surface reflectance values across the planet, through atmospheric radiative transfer models (i.e. summer, tropical). To perform this correction, we also need local atmospheric visibility and image altitude. There are many tools for this purpose:\n\nPaid: ACORN, FLAASH, QUAC, ATCOR\nFree: SMAC, Orfeo Toolbox\n\n\n\n\n\nExample of LEDAPS atmospheric correction. (a) Top-of-atmosphere (TOA) reflectance composite (bands 3,2,1) for Landsat-7 ETM+ image of San Francisco Bay (July 7, 1999); (b) Surface reflectance composite.\n\n\nFinally, atmospheric correction to obtain true reflectance is not always necessary, for example, for classification of a single image, working on composite images, etc.\n\n\n3.1.2 Geometric and topographic corrections\nSubsets of Georectification"
  },
  {
    "objectID": "week3.html#in-practice",
    "href": "week3.html#in-practice",
    "title": "3  Corrections and Enhancements",
    "section": "3.2 In practice:",
    "text": "3.2 In practice:"
  },
  {
    "objectID": "week3.html#in-short",
    "href": "week3.html#in-short",
    "title": "3  Corrections and Enhancements",
    "section": "3.3 In short…",
    "text": "3.3 In short…"
  },
  {
    "objectID": "glossary.html",
    "href": "glossary.html",
    "title": "Glossary: Demystifying jargons",
    "section": "",
    "text": "Digital Number (DN) : raw brightness data captured by sensors without having units for efficient storage\nRadiance : how much light the instrument sees in meaningful units but still have effects of light source, atmosphere and surface material. Also known as Top of Atmosphere (TOA) radiance\nReflectance : a property of a material, it is a ratio of the amount of light leaving a target to the amount of light striking it. Its calculation is performed on radiance corrected data. Typically this comes as surface (BOA) reflectance but can also be Top of Atmosphere (TOA) reflectance if the atmospheric effect on radiance has not been accounted for.\nRadiometric calibration : the process from converting DN to Radiance Lλ=Bias+(Gain∗DN)\nDark object subtraction (DOS) : A relative Atmospheric correction technique. Searches each band for the darkest value then subtracts that from each pixel. This is appropriate when the visible bands have increased scattering vs. longer wavelengths. Also known as, Histogram Adjustment\n\n\n\nPseudo-invariant Features (PIFs) : A relative Atmospheric correction technique. Assume brightness pixels linearly related to a base image (linear regression) and adjust the image based on the regression result\nAtmospheric radiative transfer models."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "“DestinE for Human Heat Stress: ECMWF\nUse Case to Tackle Urban Heat Islands.” n.d. Accessed January 29,\n2024. https://stories.ecmwf.int/destine-for-human-heat-stress-ecmwf-use-case-to-tackle-urban-heat-islands/.\n\n\nEarth Science Data Systems, NASA. 2019. “What Is Remote\nSensing? | Earthdata.” Backgrounder. August\n23, 2019. https://www.earthdata.nasa.gov/learn/backgrounders/remote-sensing.\n\n\n“Remote Sensing, Satellite Imaging\nTechnology | Satellite Imaging Corp.” n.d.\nAccessed January 27, 2024. https://www.satimagingcorp.com/services/resources/characterization-of-satellite-remote-sensing-systems/.\n\n\nSchumann, Guy, Laura Giustarini, Angelica Tarpanelli, Ben Jarihani, and\nSandro Martinis. 2023. “Flood Modeling and\nPrediction Using Earth Observation Data.”\nSurveys in Geophysics 44 (5): 1553–78. https://doi.org/10.1007/s10712-022-09751-y.\n\n\nWang, Haibo, Xueshuang Gong, Bingbing Wang, Chao Deng, and Qiong Cao.\n2021. “Urban Development Analysis Using Built-up Area Maps Based\non Multiple High-Resolution Satellite Data.” International\nJournal of Applied Earth Observation and Geoinformation 103\n(December): 102500. https://doi.org/10.1016/j.jag.2021.102500."
  },
  {
    "objectID": "week1.html#summary-what-is-remote-sensing",
    "href": "week1.html#summary-what-is-remote-sensing",
    "title": "1  Remote Sensing: To see the unseen",
    "section": "1.1 Summary: What is remote sensing?",
    "text": "1.1 Summary: What is remote sensing?\nRemote Sensing is the use of satellites, planes, drones, etc. as our aerial eyes, piecing together a portrait of our planet through light and data and revolutionising the way we understand and interact with it.\nSensors, well, ‘sense’ Earth in two ways: Some passively listen to sunlight reflected off our planet’s surface (i.e., passive sensors), while others actively send their own signals and capture the echoes (i.e., active sensors). By that definition, the human eye is a type of passive sensors!\n\n\n\nActive vs Passive Remote Sensing (Earth Science Data Systems 2019).\n\n\nThese sensors interpret a fascinating language of light, both invisible and visible, known as the electromagnetic spectrum.\n\n\n\nThe Electromagnetic Spectrum (EMS). Credit: NASA Science\n\n\nElectromagnetic radiation moves as waves as perpendicular electric and magnetic field with a wavelength: λ = c/v where:\n\nλ = wavelength, distance between two crests\nc = velocity of light 3 x 108 m/sec\nv = frequency, rate of oscillation (full oscillations in a time unit)\n\nDifferent materials reflect unique wavelengths in this spectrum, allowing us to identify them, like decoding DNA! More on this later…\n\n\n\nWavelength vs. Oscillation\n\n\nBut information that sensors receive isn’t just about color. Remote sensing data has its own “resolution” recipe, encompassing:\n\nSpectral: How EMS bands (i.e., the range within the EMS) the sensor can hear, revealing more detail with each additional band.\nSpatial: The size of each pixel in the image, ranging from centimeters to kilometers, offering varying levels of detail.\nTemporal: How often the sensor revisits the same area, providing a dynamic view of changes over time.\nRadiometric: The range of brightness levels captured, painting a vibrant and accurate picture.\n\nIn reality, depending on the purpose, each sensor is equip to have better resolution of one type over the other. For example, sensors with a high spatial resolution of 5m (i.e., each pixel is 10x10 on the ground) will have lower spectral resolution (i.e., capturing only narrow range of the EMS) (“Remote Sensing, Satellite Imaging Technology | Satellite Imaging Corp” n.d.)"
  },
  {
    "objectID": "week1.html#applications-urban-analytics-use-cases",
    "href": "week1.html#applications-urban-analytics-use-cases",
    "title": "1  Remote Sensing: To see the unseen",
    "section": "1.2 Applications: Urban Analytics use cases",
    "text": "1.2 Applications: Urban Analytics use cases\nRemote Sensing has many transformative applications in the realm of Urban Analytics. Think of it as an X-ray for cities, revealing hidden patterns and empowering informed decision-making by city planners, urban designers, and public officials to make swift and informed decisions to improve the lives of millions of urbanites.\nHere are some examples\n\nMapping Urban Growth: By tracking changes in the built environment over time, we can identify sprawling suburbs, monitor urban expansion, and plan for infrastructure needs. Remote sensing data are particularly rich sources to train classification models to extract features and identify unmapped communities.\n\n\n\n\nTransferable built-up area extraction (TBUAE) framework to map urbanised areas (Wang et al. 2021)\n\n\n\nPredicting Floods with Foresight: Analysing land cover and terrain, it anticipates where water will flow, safeguarding communities from harm, while also estimating potential damage. Here is an example of how insurance companies can make use of such data to assess risk and process claims.\n\n\n\n\nRemote sensing data can be used to estimate flood extent, and to derive individual risk level damage estimates (Schumann et al. 2023)\n\n\n\nEnergy Efficiency: Identifying heat island effects and understanding building energy consumption through thermal imaging empowers planners to design sustainable cities, and researchers to determine the best strategies to combat long term climate disasters.\n\n\n\n\nLeft: Average 2m air temperature at 23h (moment of max UHI) during all summer months (June-August) of the years 1987- 2016. Right: UrbCLIM output field downscaled to 30-m resolution showing the average daily maximum urban heat island (UHI) intensity for a part of the city of Amsterdam. (“DestinE for Human Heat Stress: ECMWF Use Case to Tackle Urban Heat Islands” n.d.)\n\n\nThis is just the beginning. Remote sensing is transforming the way we understand and manage our cities, paving the way for a healthier, smarter, and more sustainable urban future."
  },
  {
    "objectID": "week1.html#reflection",
    "href": "week1.html#reflection",
    "title": "1  Intro to Remote Sensing",
    "section": "1.3 Reflection",
    "text": "1.3 Reflection\nMy first foray into the world of Remote Sensing was eye-opening, to say the least. For the uninitiated, it is easy to assume that remote sensing purely means orthophotographic satellite images that one might see using platforms such as Google Maps, i.e., as if the only thing that sensors do were to snap a simple photo of the planet like a phone camera.\nIn reality, it unlocks unseen depths of data beyond mere human perception, not just high-resolution photographs and more spectral signatures, where each pixel holds a universe of information. Building a true-color composite from all these layers (so that our limited human vision can perceive them) was like seeing the world through a brand-new lens.\nI am particularly excited to get started with Google Earth Engine later on as the primary gateway to access the wealth of remote sensing data and analytics with more ease in order to solve specific problems facing our world today.\n\n\n\n\n“DestinE for Human Heat Stress: ECMWF Use Case to Tackle Urban Heat Islands.” n.d. Accessed January 29, 2024. https://stories.ecmwf.int/destine-for-human-heat-stress-ecmwf-use-case-to-tackle-urban-heat-islands/.\n\n\nEarth Science Data Systems, NASA. 2019. “What Is Remote Sensing? | Earthdata.” Backgrounder. August 23, 2019. https://www.earthdata.nasa.gov/learn/backgrounders/remote-sensing.\n\n\n“Remote Sensing, Satellite Imaging Technology | Satellite Imaging Corp.” n.d. Accessed January 27, 2024. https://www.satimagingcorp.com/services/resources/characterization-of-satellite-remote-sensing-systems/.\n\n\nSchumann, Guy, Laura Giustarini, Angelica Tarpanelli, Ben Jarihani, and Sandro Martinis. 2023. “Flood Modeling and Prediction Using Earth Observation Data.” Surveys in Geophysics 44 (5): 1553–78. https://doi.org/10.1007/s10712-022-09751-y.\n\n\nWang, Haibo, Xueshuang Gong, Bingbing Wang, Chao Deng, and Qiong Cao. 2021. “Urban Development Analysis Using Built-up Area Maps Based on Multiple High-Resolution Satellite Data.” International Journal of Applied Earth Observation and Geoinformation 103 (December): 102500. https://doi.org/10.1016/j.jag.2021.102500."
  },
  {
    "objectID": "week3.html#summary",
    "href": "week3.html#summary",
    "title": "3  Corrections and Enhancements",
    "section": "3.1 Summary",
    "text": "3.1 Summary\nRaw data coming from sensors are rarely immediately usable without being corrected for various interferences and effects. Common correction methods are geometric and topographic corrections, and radiometric and atmospheric corrections. Finally, enhancement methods help us highlight values of interest that pertain to our research scope. Good news: Standard Remote sensing products now come corrected, and are usually labeled “Analysis Ready Data” or ARD.\nRemember: Radiometric/atmospheric corrections happen before geometric/topographic correction.\n\n3.1.1 Radiometric and Atmospheric corrections\nThese processes describe translating raw light data from the sensor into ‘true’ information on the surface’s reflectance property, without interference from the light source and the atmosphere.\n\nRadiometric calibration is the conversion from raw Digital Number to Spectral Radiance via a linear transformation \\(L_λ=Bias+(Gain∗DN)\\). Radiance most often has units of watt/(steradian/square meter)\nAtmospheric correction involves the next step:\n\nTOA Radiance-to-Reflectance correction removes effects of the light source (e.g. the sun) by calibrating radiation going down (irradiance) and up (radiance). TOA Reflectance still has the effect of the atmosphere and the surface material. If irradiance = radiance, we call this hemispheric reflectance\nTOA-to-BOA Reflectance correction removes effects of the atmospheric conditions, leaving us with just data on the surface materials. If shadows and directional effects on reflectance have been dealt with, we get what is called true reflectance, if not then it is called apparent reflectance.\n\n\nAtmospheric correction deserves our attention considering the effect of atmospheric scattering on the final results. Absorption and scattering create the haze which reduces the contrast, and can creates the “adjacency effect”, whereby radiance from pixels nearby mixed into pixel of interest. Atmospheric correction to obtain true reflectance is not always necessary, for example, for classification of a single image, working on composite images, etc.\nThere are two types of atmospheric correction:\n\nRelative: normalise intensities of different bands within a single image, or from many dates to one date. This can be done via Dark Object Subtraction (DOS), or Pseudo-invariant Features (PIFs)\nAbsolute: change digital brightness values into scaled surface reflectance. We can then compare these scaled surface reflectance values across the planet, through atmospheric radiative transfer models (i.e. summer, tropical), or Empirical Line Correction\n\n\n\n\nExample of LEDAPS atmospheric correction. (a) Top-of-atmosphere (TOA) reflectance composite (bands 3,2,1) for Landsat-7 ETM+ image of San Francisco Bay (July 7, 1999); (b) Surface reflectance composite.\n\n\n\n\n3.1.2 Geometric and topographic corrections\nThese are subsets of Georectification which gives coordinates to an image, and accounting for view angel, topography, wind disturbance, and rotation of the Earth, etc., which distort the geometry of the resultant image.\nTopographic correction corrects the view angle of the image so that it is nadir (i.e., directly top-down). Important concepts to get familiar with for orthorectification are:\n\nSolar azimuth: compass angle of the sun (N =0°) 90° (E) at sunrise and 270° (W) at sunset.\nSolar zenith: angle of local zenith (above the point on ground) and sun from vertical (90° - elevation)\n\n\n\n\n\n\nGeometric correction effectively ‘grounds’ the images into a georeferenced final product (i.e., with a coordinate). In order to do so, we identify Ground Control Points (GPS) to match known points in the image and a reference dataset. We then take the coordinates and model them to give geometric transformation coefficients (linear regression). It effectively resembles fitting old maps into a digital version.\n\n\n\n\n\n\n\n3.1.3 Joining and Enhancements\nTo join (i.e., ‘mosaicking’), within the overlap area (20-30%) an representative sample is taken, a histogram is extracted from the base image which is then applied to other image to using a histogram matching algorithm to blend the brightness values of the two images (‘feathering’)\nFinally, after all the corrections, there are still many enhancements that can improve or accentuate the visual results depending on the purpose of the research:\n\nContrast enhancement to accentuate reflectance values that are close to each other\nRatio calculation calculates pixel value as a ratio of different band (e.g. Normalised Burn Ratio)\nFiltering is the use of low-pass filters that average (i.e., smooth) the data, or high-pass filters that enhance the variances between features. Filtering is used to perform texture and edge detection.\nPCA transforms multi-spectral data into uncorrelated dataset. Multi-date overlay PCA is a way to detect change efficiently.\nFusion entails fusing image/data from multiple sensors in order to improve details, enable better classification, or downsample."
  },
  {
    "objectID": "week1.html#applications-use-cases-in-urban-analytics",
    "href": "week1.html#applications-use-cases-in-urban-analytics",
    "title": "1  Remote Sensing: To see the unseen",
    "section": "1.2 Applications: Use cases in Urban Analytics",
    "text": "1.2 Applications: Use cases in Urban Analytics\nRemote Sensing has many transformative applications in the realm of Urban Analytics. Think of it as an X-ray for cities, revealing hidden patterns and empowering informed decision-making by city planners, urban designers, and public officials to make swift and informed decisions to improve the lives of millions of urbanites.\nHere are some examples\n\nMapping Urban Growth: By tracking changes in the built environment over time, we can identify sprawling suburbs, monitor urban expansion, and plan for infrastructure needs. Remote sensing data are particularly rich sources to train classification models to extract features and identify unmapped communities.\n\n\n\n\nTransferable built-up area extraction (TBUAE) framework to map urbanised areas (Wang et al. 2021)\n\n\n\nPredicting Floods with Foresight: Analysing land cover and terrain, it anticipates where water will flow, safeguarding communities from harm, while also estimating potential damage. Here is an example of how insurance companies can make use of such data to assess risk and process claims.\n\n\n\n\nRemote sensing data can be used to estimate flood extent, and to derive individual risk level damage estimates (Schumann et al. 2023)\n\n\n\nEnergy Efficiency: Identifying heat island effects and understanding building energy consumption through thermal imaging empowers planners to design sustainable cities, and researchers to determine the best strategies to combat long term climate disasters.\n\n\n\n\nLeft: Average 2m air temperature at 23h (moment of max UHI) during all summer months (June-August) of the years 1987- 2016. Right: UrbCLIM output field downscaled to 30-m resolution showing the average daily maximum urban heat island (UHI) intensity for a part of the city of Amsterdam. (“DestinE for Human Heat Stress: ECMWF Use Case to Tackle Urban Heat Islands” n.d.)\n\n\nThis is just the beginning. Remote sensing is transforming the way we understand and manage our cities, paving the way for a healthier, smarter, and more sustainable urban future."
  },
  {
    "objectID": "week3.html#application",
    "href": "week3.html#application",
    "title": "3  Corrections and Enhancements",
    "section": "3.2 Application",
    "text": "3.2 Application"
  },
  {
    "objectID": "week3.html#reflection",
    "href": "week3.html#reflection",
    "title": "3  Corrections and Enhancements",
    "section": "3.3 Reflection",
    "text": "3.3 Reflection"
  },
  {
    "objectID": "week1.html#summary",
    "href": "week1.html#summary",
    "title": "1  Intro to Remote Sensing",
    "section": "1.1 Summary",
    "text": "1.1 Summary\nRemote Sensing is the use of satellites, planes, drones, etc. as our aerial eyes, piecing together a portrait of our planet through light and data and revolutionising the way we understand and interact with it.\nSensors, well, ‘sense’ Earth in two ways: Some passively listen to sunlight reflected off our planet’s surface (i.e., passive sensors), while others actively send their own signals and capture the echoes (i.e., active sensors). By that definition, the human eye is a type of passive sensors!\n\n\n\nActive vs Passive Remote Sensing (Earth Science Data Systems 2019).\n\n\nThese sensors interpret a fascinating language of light, both invisible and visible, known as the electromagnetic spectrum.\n\n\n\nThe Electromagnetic Spectrum (EMS). Credit: NASA Science\n\n\nElectromagnetic radiation moves as waves as perpendicular electric and magnetic field with a wavelength: λ = c/v where:\n\nλ = wavelength, distance between two crests\nc = velocity of light 3 x 108 m/sec\nv = frequency, rate of oscillation (full oscillations in a time unit)\n\nDifferent materials reflect unique wavelengths in this spectrum, allowing us to identify them, like decoding DNA!\n\n\n\nWavelength vs. Oscillation\n\n\nBut information that sensors receive isn’t just about color. Remote sensing data has its own “resolution” recipe, encompassing:\n\nSpectral: How EMS bands (i.e., the range within the EMS) the sensor can hear, revealing more detail with each additional band.\nSpatial: The size of each pixel in the image, ranging from centimeters to kilometers, offering varying levels of detail.\nTemporal: How often the sensor revisits the same area, providing a dynamic view of changes over time.\nRadiometric: The range of brightness levels captured, painting a vibrant and accurate picture.\n\nIn reality, depending on the purpose, each sensor is equip to have better resolution of one type over the other. For example, sensors with a high spatial resolution of 5m (i.e., each pixel is 10x10 on the ground) will have lower spectral resolution (i.e., capturing only narrow range of the EMS) (“Remote Sensing, Satellite Imaging Technology | Satellite Imaging Corp” n.d.)"
  },
  {
    "objectID": "week1.html#application",
    "href": "week1.html#application",
    "title": "1  Intro to Remote Sensing",
    "section": "1.2 Application",
    "text": "1.2 Application\nRemote Sensing has many transformative applicatioicns in the realm of Urban Analytics. Think of it as an X-ray for cities, revealing hidden patterns and empowering informed decision-making by city planners, urban designers, and public officials to make swift and informed decisions to improve the lives of millions of urbanites.\nHere are some examples of the use of Remote Sensing in urban analytics research\n\nMapping Urban Growth: By tracking changes in the built environment over time, we can identify sprawling suburbs, monitor urban expansion, and plan for infrastructure needs. Recent works have made use of a diverse source of high-resolution Remote Sensing data (2m) to train machine-learning model to more robustly extract ‘urbanised’ areas compared to previous methodology using only medium-resolution images. (Wang et al. 2021)\n\n\n\n\nTransferable built-up area extraction (TBUAE) framework to map urbanised areas\n\n\n\nPredicting Floods with Foresight: Analysing land cover and terrain, it anticipates where water will flow, safeguarding communities from harm, while also estimating potential damage. This type of risk assessment is highly relevant not only in climate science and the public sector but also among insurance companies who make use of SAR satellite data (for through-cloud vision) combined with other spatial data sets to assess risk and process claims (Schumann et al. 2023)\n\n\n\n\nRemote sensing data can be used to estimate flood extent, and to derive individual risk level damage estimates\n\n\n\nEnergy Efficiency: Identifying heat island effects and understanding building energy consumption through thermal imaging empowers planners to design sustainable cities, and researchers. Remote Sensing data is a key component in Urban Climate Models such as UrbCLIM by VITO, a Belgian research organisation, who aims to create an interactive tool providing high resolution urban heat maps and predict their evolution under future climate conditions. (“DestinE for Human Heat Stress: ECMWF Use Case to Tackle Urban Heat Islands” n.d.)\n\n\n\n\nLeft: Average 2m air temperature at 23h (moment of max Urban Heat Islands) during all summer months of the years 1987- 2016. Right: UrbCLIM climate model’s output field showing the average daily maximum urban heat island intensity for Amsterdam\n\n\nThis is just the beginning. Remote sensing is transforming the way we understand and manage our cities, paving the way for a healthier, smarter, and more sustainable urban future."
  }
]